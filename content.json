{"meta":{"title":"CY","subtitle":"","description":"CY的博客","author":"CY","url":"https://yao-chen-ecnu.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2021-03-15T08:24:55.051Z","updated":"2021-03-15T08:24:55.051Z","comments":false,"path":"/404.html","permalink":"https://yao-chen-ecnu.github.io/404.html","excerpt":"","text":""},{"title":"书单","date":"2021-03-15T08:24:55.051Z","updated":"2021-03-15T08:24:55.051Z","comments":false,"path":"books/index.html","permalink":"https://yao-chen-ecnu.github.io/books/index.html","excerpt":"","text":""},{"title":"关于","date":"2021-03-15T08:24:55.051Z","updated":"2021-03-15T08:24:55.051Z","comments":false,"path":"about/index.html","permalink":"https://yao-chen-ecnu.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"Repositories","date":"2021-03-15T08:24:55.056Z","updated":"2021-03-15T08:24:55.056Z","comments":false,"path":"repository/index.html","permalink":"https://yao-chen-ecnu.github.io/repository/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-03-15T08:24:55.056Z","updated":"2021-03-15T08:24:55.056Z","comments":true,"path":"links/index.html","permalink":"https://yao-chen-ecnu.github.io/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-03-15T08:24:55.056Z","updated":"2021-03-15T08:24:55.056Z","comments":false,"path":"tags/index.html","permalink":"https://yao-chen-ecnu.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2021-03-21T06:28:01.744Z","updated":"2021-03-15T08:24:55.056Z","comments":false,"path":"categories/index.html","permalink":"https://yao-chen-ecnu.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Distilling the Knowledge in a Neural Network","slug":"Distilling the Knowledge in a Neural Network","date":"2021-06-22T02:47:54.194Z","updated":"2021-06-23T01:31:15.699Z","comments":true,"path":"2021/06/22/Distilling the Knowledge in a Neural Network/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/06/22/Distilling%20the%20Knowledge%20in%20a%20Neural%20Network/","excerpt":"","text":"Abstract 提高机器学习算法表现的一个简单方法就是，训练不同模型然后对预测结果取平均。 但是要训练多个模型会带来过高的计算复杂度和部署难度。 可以将集成的知识压缩在单一的模型中。 论文使用这种方法在MNIST上做实验，发现取得了不错的效果。 论文还介绍了一种新型的集成，包括一个或多个完整模型和专用模型，能够学习区分完整模型容易混淆的细粒度的类别。 1 Introduction 昆虫有幼虫期和成虫期，幼虫期主要行为是吸收养分，成虫期主要行为是生长繁殖。 类似地，大规模机器学习应用可以分为训练阶段和部署阶段，训练阶段不要求实时操作，允许训练一个复杂缓慢的模型，这个模型可以是分别训练多个模型的集成，也可以是单独的一个很大的带有强正则比如dropout的模型。 一旦模型训练好，可以用不同的训练，这里称为“蒸馏”，去把知识转移到更适合部署的小模型上。 复杂模型学习区分大量的类，通常的训练目标是最大化正确答案的平均log概率，这么做有一个副作用就是训练模型同时也会给所有的错误答案分配概率，即使这个概率很小，而有一些概率会比其它的大很多。错误答案的相对概率体现了复杂模型的泛化能力。举个例子，宝马的图像被错认为垃圾箱的概率很低，但是这被个错认为垃圾桶的概率相比于被错认为胡萝卜的概率来说，是很大的。（可以认为模型不止学到了训练集中的宝马图像特征，还学到了一些别的特征，比如和垃圾桶共有的一些特征，这样就可能捕捉到在新的测试集上的宝马出现这些的特征，这就是泛化能力的体现） 将复杂模型转为小模型需要保留模型的泛化能力，一个方法就是用复杂模型产生的分类概率作为“软目标”来训练小模型。 当软目标的熵值较高时，相对于硬目标，每个训练样本提供更多的信息，训练样本之间会有更小的梯度方差。 所以小模型经常可以被训练在小数据集上，而且可以使用更高的学习率。 像MNIST这种分类任务，复杂模型可以产生很好的表现，大部分信息分布在小概率的软目标中。 为了规避这个问题，Caruana和他的合作者们使用softmax输出前的units值，而不是softmax后的概率，最小化复杂模型和简单模型的units的平方误差来训练小模型。 而更通用的方法，蒸馏法，先提高softmax的温度参数直到模型能产生合适的软目标。然后在训练小模型匹配软目标的时候使用相同的温度T。 被用于训练小模型的转移训练集可以包括未打标签的数据（可以没有原始的实际标签，因为可以通过复杂模型获取一个软目标作为标签），或者使用原始的数据集，使用原始数据集可以得到更好的表现。 2 Distillation 原始softmax公式： 但要是直接使用softmax层的输出值作为soft target, 这又会带来一个问题: 当softmax输出的概率分布熵相对较小时，负标签的值都很接近0，对损失函数的贡献非常小，小到可以忽略不计。因此**“温度”**这个变量就派上了用场。 下面的公式时加了温度这个变量之后的softmax函数: T越大可以得到更&quot;软&quot;的概率分布。 （T越大，不同激活值的概率差异越小，所有激活值的概率趋于相同；T越小，不同激活值的概率差异越大) （在蒸馏训练的时候使用较大的T的原因是，较小的T对于那些远小于平均激活值的单元会给予更少的关注，而这些单元是有用的，使用较高的T能够捕捉这些信息） 如上图所示，教师网络（左侧）的预测输出除以温度参数（Temperature）之后、再做softmax变换，可以获得软化的概率分布（软目标或软标签），数值介于0~1之间，取值分布较为缓和。 Temperature数值越大，分布越缓和；而Temperature数值减小，容易放大错误分类的概率，引入不必要的噪声。针对较困难的分类或检测任务，Temperature通常取1，确保教师网络中正确预测的贡献。 硬目标则是样本的真实标注，可以用one-hot矢量表示。 total loss设计为软目标与硬目标所对应的交叉熵的加权平均（表示为KD loss与CE loss），其中软目标交叉熵的加权系数越大，表明迁移诱导越依赖教师网络的贡献，这对训练初期阶段是很有必要的，有助于让学生网络更轻松的鉴别简单样本，但训练后期需要适当减小软目标的比重，让真实标注帮助鉴别困难样本。 另外，教师网络的推理性能通常要优于学生网络，而模型容量则无具体限制，且教师网络推理精度越高，越有利于学生网络的学习。 3 Preliminary experiments on MNIST net layers units of each layer activation regularization test errors single net1 2 1600 relu dropout 67 single net2 2 800 relu no 146 net large net small net temperature test errors distilled net single net1 single net2 20 74 在MNIST这个数据集上，先使用大的网络进行训练，测试集错误67个，小网络训练，测试集错误146个。加入soft targets到目标函数中，相当于正则项，测试集的错误降低到了74个。这证明了teacher网络确实把知识转移到了student网络，使得结果变好了。 4 Experiments on speech recognition 研究了用于自动语音识别的深度神经网络(DNN)声学模型的集成效果。我们表明，我们在本文中提出的提取策略实现了将一组模型提取到单个模型中的预期效果，该模型比直接从相同训练数据中学习的相同大小的模型工作得更好。 system Test Frame Accuracy Word Error Rate on dev set baseline 58.9% 10.9% 10XEnsemble 61.1% 10.7% Distilled model 60.8% 10.7% 其中basline的配置为 8 层，每层2560个relu单元 softmax层的单元数为14000 训练样本大小约为 700M，2000个小时的语音文本数据 10XEnsemble是对baseline训练10次（随机初始化为不同参数）然后取平均 蒸馏模型的配置为 使用的候选温度为{1, 2, 5, 10}, 其中T为2时表现最好 hard target 的目标函数给予0.5的相对权重 使用不同的参数训练了10个DNN，对这10个模型的预测结果求平均作为emsemble的结果，相比于单个模型有一定的提升。然后将这10个模型作为teacher网络，训练student网络。得到的Distilled Single model相比于直接的单个网络，也有一定的提升 5 Training ensembles of specialists on very big datasets 训练一个大的集成模型可以利用并行计算来训练，训练完成后把大模型蒸馏成小模型，但是另一个问题就是，训练本身就要花费大量的时间，这一节介绍的是如何学习专用模型集合，集合中的每个模型集中于不同的容易混淆的子类集合，这样可以减小计算需求。专用模型的主要问题是容易集中于区分细粒度特征而导致过拟合，可以使用软目标来防止过拟合。 5.1 JFT数据集 JFT是一个谷歌的内部数据集，有1亿的图像，15000个标签。google用一个深度卷积神经网络，训练了将近6个月。 我们需要更快的方法来提升baseline模型。 5.2 专用模型 将一个复杂模型分为两部分，一部分是一个用于训练所有数据的通用模型，另一部分是很多个专用模型，每个专用模型训练的数据集是一个容易混淆的子类集合。这些专用模型的softmax结合所有不关心的类为一类来使模型更小。 为了减少过拟合，共享学习到的低水平特征，每个专用模型用通用模型的权重进行初始化。另外，专用模型的训练样本一半来自专用子类集合，另一半从剩余训练集中随机抽取。 5.3 将子类分配到专用模型 专用模型的子类分组集中于容易混淆的那些类别，虽然计算出了混淆矩阵来寻找聚类，但是可以使用一种更简单的办法，不需要使用真实标签来构建聚类。对通用模型的预测结果计算协方差，根据协方差把经常一起预测的类作为其中一个专用模型的要预测的类别。几个简单的例子如下。 JFT 1: Tea party; Easter; Bridal shower; Baby shower; Easter Bunny; … JFT 2: Bridge; Cable-stayed bridge; Suspension bridge; Viaduct; Chimney; … JFT 3: Toyota Corolla E100; Opel Signum; Opel Astra; Mazda Familia; … 5.4 实验表现 system Conditional Test Accuracy Test Accuracy baseline 43.1% 25.0% 61 specialist models 45.9% 26.1% 6 Soft Targets as Regularizers 我们关于使用软目标而不是硬目标的主要主张之一是，许多有用的信息可以在软目标中携带，而这些信息不可能用单个硬目标编码。表5显示，只有3%的数据(约20M个例子)，用硬目标训练基线模型会导致严重的过度拟合(我提前停止了训练，因为精度在达到44.5%后急剧下降)，而用软目标训练的相同模型能够恢复完整训练集中的几乎所有信息(约2%)。更值得注意的是，我们不需要提前停止:具有软目标的系统简单地“收敛”到57%。这表明，软目标是一种非常有效的方式，可以将模型在所有数据上发现的规律传递给另一个模型。。 system &amp; training set Train Frame Accuracy Test Frame Accuracy baseline(100% training set) 63.4% 58.9% baseline(3% training set) 67.3% 44.5% soft targets(3% training set) 65.4% 57.0% 简要总结 主要工作 “蒸馏”（distillation）：把大网络的知识压缩成小网络的一种方法 “专用模型”（specialist models）：对于一个大网络，可以训练多个专用网络来提升大网络的模型表现 具体做法 蒸馏：先训练好一个大网络，在最后的softmax层使用合适的温度参数T，最后训练得到的概率称为“软目标”。以这个软目标和真实标签作为目标，去训练一个比较小的网络，训练的时候也使用在大模型中确定的温度参数T 专用模型：对于一个已经训练好的大网络，可以训练一系列的专用模型，每个专用模型只训练一部分专用的类以及一个“不属于这些专用类的其它类”，比如专用模型1训练的类包括“显示器”，“鼠标”，“键盘”，…，“其它”；专用模型2训练的类包括“玻璃杯”，“保温杯”，“塑料杯”，“其它“。最后以专用模型和大网络的预测输出作为目标，训练一个最终的网络来拟合这个目标。 意义 蒸馏把大网络压成小网络，这样就可以先在训练阶段花费大精力训练一个大网络，然后在部署阶段以较小的计算代价来产生一个较小的网络，同时保持一定的网络预测表现。 对于一个已经训练好的大网络，如果要去做集成的话计算开销是很大的，可以在这个基础上训练一系列专用模型，因为这些模型通常比较小，所以训练会快很多，而且有了这些专用模型的输出可以得到一个软目标，实验证明使用软目标训练可以减小过拟合。最后根据这个大网络和一系列专用模型的输出作为目标，训练一个最终的网络，可以得到不错的表现，而且不需要对大网络做大量的集成计算。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://yao-chen-ecnu.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"知识蒸馏","slug":"知识蒸馏","permalink":"https://yao-chen-ecnu.github.io/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"}]},{"title":"DistillE","slug":"DistilE","date":"2021-06-22T02:47:54.190Z","updated":"2021-06-23T01:35:13.624Z","comments":true,"path":"2021/06/22/DistilE/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/06/22/DistilE/","excerpt":"","text":"Abstract 知识图嵌入(KGE)是一种流行的KG推理方法，通常高维度的知识图嵌入可以保证更好的推理能力。然而，高维数据对存储和计算资源提出了巨大的挑战，不适合资源受限或时间受限的应用，因此需要更快、更便宜的推理。**为了解决这个问题，我们提出了蒸馏法，一种从预先训练好的高维老师KGE那里建立低维学生KGE的知识蒸馏方法。**我们将原始的KGE损耗作为硬标签损失，针对蒸馏过程中不同类型的KGE设计了特定的软标签损失。我们还提出了一个两阶段的提炼方法，使学生和老师相互适应，进一步提高学生的推理能力。 相关预测实验结果表明，该方法成功蒸馏得到一个好学生，其性能优于直接训练的同维学生，有时甚至优于教师，并且在性能损失较小的情况下，可以达到2倍至8倍的嵌入压缩率，推理速度比教师快10倍以上。我们还通过消融研究实验证明了我们的两阶段训练方案的有效性。 Introduction 为了获得更好的性能，通常首选训练更高维数的KGE。而模型的大小，即参数的数量和推理时间的成本通常随着嵌入维数的增加而快速增加。如图1所示，随着嵌入维数变大，模型(MRR)的性能增长越来越慢，而模型大小和推理成本增长仍然很快。 然而，高维嵌入在许多现实生活场景中是不切实际的。例如，一个预先训练的十亿规模的知识图谱被期望被微调以解决下游任务，并以更低的成本频繁部署。对于计算资源有限的应用程序，如在边缘计算或移动设备上部署KG，或者推理时间有限的应用程序，如在线财务预测，较低维度的嵌入提供了明显甚至不可或缺的便利。虽然低维度能够实现更快的部署和更便宜的推理，但是直接用小嵌入量进行训练通常表现不佳，如图1所示。因此，我们提出了一个新的研究问题:是否有可能从预先训练好的高维数据中提取低维的kge，以便只要更快和更便宜的推理就能获得好的性能？ 知识蒸馏是一种从大模型(教师)中提取知识以构建小模型(学生)的技术，已在计算机视觉和自然语言处理领域得到广泛研究。学生从硬标签(事实标签)和老师的软标签中学习。在这项工作中，我们提出了一种新的大规模知识图谱训练蒸馏方法，称为DistilE，它能够从高维KGE将知识提取到一个更小的嵌入中去，而不会损失太多的准确性，同时表现要比直接用相同小的嵌入直接去训练好的多。 我们还提出了一种两阶段蒸馏方法来进一步改善蒸馏结果。基本的想法是，虽然受过训练的老师已经很强了，但如果老师也能向学生学习，而不是一直固定不变，就可以取得更好的成绩。因此，除了一个标准的蒸馏阶段，在这个阶段中，老师总是静止的，我们设计了第二个蒸馏阶段，在这个阶段中，老师被解冻，并试图调整自己从而变得更容易被学生接受。 实验结果证明了该方法的有效性，结果表明: (1)蒸馏得到的低维嵌入比直接训练相同大小的嵌入要好得多 (2)蒸馏得到的低维嵌入比原始的高维嵌入要快得多 (3)两阶段蒸馏方法效果良好，可以进一步提高蒸馏结果。 主要贡献： 我们提出了一个新的框架来从高维KGE中提取低维的KGE。这是第一次将知识蒸馏应用于知识图嵌入。 我们为不同种类的KGE设计不同的软标签，并采用两阶段蒸馏来提高蒸馏效果。 我们通过实验证明，我们可以将KGE的参数数量减少8倍，并将推理速度提高10倍以上，同时保持良好的性能。 Method 首先，我们为不同类型的KGE设计不同的蒸馏目标。然后，我们介绍我们的两阶段蒸馏方法，以不断调整教师更好的蒸馏结果。 蒸馏目标 给定一个KG K={E,R,T}\\mathcal{K}=\\{E, R, T\\}K={E,R,T}，其中E，R，T分别是实体，关系，三元组的集合。具体来说，对于三元组(h,r,t)(h,r,t)(h,r,t)，KGE模型可以通过分数函数fr(h,t)f_{r}(h, t)fr​(h,t)得到嵌入的分数，以表示嵌入的效果。不同评分函数的KGE有不同的训练目标和相应的损失函数。两种最常用的KGE损失包括边界损失和交叉熵损失。对于不同类型的损失，需要不同类型的蒸馏目标。 Objective for KGEs with Marginal Loss 边界损失通常适用于基于翻译的KGE，包括TransE,TransH, TransR, and TransD等。 评分函数fr(h,t)f_{r}(h, t)fr​(h,t)是基于距离指标来评价一个嵌入(h,r,t)(h,r,t)(h,r,t) 训练目标是使fr(h,t)f_{r}(h, t)fr​(h,t)对正三元组(h,r,t)(h,r,t)(h,r,t)较小，对负三元组(h,r,t)(h,r,t)(h,r,t)较大，并迫使正、负三元组分数之间的差值大于一个差值。 教师的损失：Lhard T=∑(h,r,t)∈G[frT(h,t)−frT(h′,t′)+γ]+L_{\\text {hard }}^{T}=\\sum_{(h, r, t) \\in G}\\left[f_{r}^{T}(h, t)-f_{r}^{T}\\left(h^{\\prime}, t^{\\prime}\\right)+\\gamma\\right]_{+}Lhard T​=∑(h,r,t)∈G​[frT​(h,t)−frT​(h′,t′)+γ]+​ frT(h,t)f_{r}^{T}(h, t)frT​(h,t)是正样本(h,r,t)(h,r,t)(h,r,t)的评分，frT(h′,t′)f_{r}^{T}\\left(h^{\\prime}, t^{\\prime}\\right)frT​(h′,t′)是负样本(h′,r,t′)\\left(h^{\\prime}, r, t^{\\prime}\\right)(h′,r,t′)的评分 学生的硬标签损失：Lhard S=∑(h,r,t)∈G[frS(h,t)−frS(h′,t′)+γ]+L_{\\text {hard }}^{S}=\\sum_{(h, r, t) \\in G}\\left[f_{r}^{S}(h, t)-f_{r}^{S}\\left(h^{\\prime}, t^{\\prime}\\right)+\\gamma\\right]_{+}Lhard S​=∑(h,r,t)∈G​[frS​(h,t)−frS​(h′,t′)+γ]+​ 学生的软标签损失： 由于这些KGE像传统的知识蒸馏方法一样缺乏必要的概率输出层，拟合概率分布在这里不适用。一个自然的选择是使用老师的嵌入评分作为学生的软标签，因为分数包含了关于三元组真实性的更丰富的信息。然后，我们让学生通过最小化这两个分数之间的差异来适应老师。 Lsoft S=∑(h,r,t)∈G(∣frT(h,t)−frS(h,t)∣+∑i=1k∣frT(hi′,ti′)−frS(hi′,ti′)∣)\\begin{aligned} L_{\\text {soft }}^{S}=\\sum_{(h, r, t) \\in G}(&amp;\\left|f_{r}^{T}(h, t)-f_{r}^{S}(h, t)\\right| \\\\ &amp;\\left.+\\sum_{i=1}^{k}\\left|f_{r}^{T}\\left(h_{i}^{\\prime}, t_{i}^{\\prime}\\right)-f_{r}^{S}\\left(h_{i}^{\\prime}, t_{i}^{\\prime}\\right)\\right|\\right) \\end{aligned} Lsoft S​=(h,r,t)∈G∑​(​∣∣​frT​(h,t)−frS​(h,t)∣∣​+i=1∑k​∣∣​frT​(hi′​,ti′​)−frS​(hi′​,ti′​)∣∣​)​ 最终损失：L=αLsoft S+(1−α)Lhard SL=\\alpha L_{\\text {soft }}^{S}+(1-\\alpha) L_{\\text {hard }}^{S}L=αLsoft S​+(1−α)Lhard S​ Objective for KGEs with Cross Entropy Loss. 交叉熵损失通常用于输出具有概率解释的模型，例如，双线性模型(如ComplEx)、旋转模型(如RotatE)和基于神经网络的模型。 教师损失： Lhard T=−∑(h,r,t)∈G∪G−(ylog⁡p(h,r,t)T+(1−y)log⁡(1−p(h,r,t)T)),\\begin{aligned} L_{\\text {hard }}^{T}=&amp;-\\sum_{(h, r, t) \\in G \\cup G^{-}}\\left(y \\log p_{(h, r, t)}^{T}\\right.\\\\ &amp;\\left.+(1-y) \\log \\left(1-p_{(h, r, t)}^{T}\\right)\\right), \\end{aligned} Lhard T​=​−(h,r,t)∈G∪G−∑​(ylogp(h,r,t)T​+(1−y)log(1−p(h,r,t)T​)),​ 其中p(h,r,t)T=exp⁡frT(h,t)1+exp⁡frT(h,t)p_{(h, r, t)}^{T}=\\frac{\\exp f_{r}^{T}(h, t)}{1+\\exp f_{r}^{T}(h, t)}p(h,r,t)T​=1+expfrT​(h,t)expfrT​(h,t)​ 学生的硬标签损失： LhardS=−∑(h,r,t)∈G∪G−(ylog⁡p(h,r,t)S+(1−y)log⁡(1−p(h,r,t)S)),\\begin{aligned} L_{h a r d}^{S}=&amp;-\\sum_{(h, r, t) \\in G \\cup G^{-}}\\left(y \\log p_{(h, r, t)}^{S}\\right.\\\\ &amp;\\left.+(1-y) \\log \\left(1-p_{(h, r, t)}^{S}\\right)\\right), \\end{aligned} LhardS​=​−(h,r,t)∈G∪G−∑​(ylogp(h,r,t)S​+(1−y)log(1−p(h,r,t)S​)),​ 学生的软标签损失： 由于这些知识的输出具有概率解释，学生的软标签损失可以定义为学生和教师输出的概率分布的交叉熵，如传统的知识提取方法: Lsoft S=−∑(h,r,t)∈G∪G−(p(h,r,t)Tlog⁡p(h,r,t)S+(1−p(h,r,t)T)log⁡(1−p(h,r,t)S))\\begin{aligned} L_{\\text {soft }}^{S}=&amp;-\\sum_{(h, r, t) \\in G \\cup G^{-}}\\left(p_{(h, r, t)}^{T} \\log p_{(h, r, t)}^{S}\\right.\\\\ &amp;\\left.+\\left(1-p_{(h, r, t)}^{T}\\right) \\log \\left(1-p_{(h, r, t)}^{S}\\right)\\right) \\end{aligned} Lsoft S​=​−(h,r,t)∈G∪G−∑​(p(h,r,t)T​logp(h,r,t)S​+(1−p(h,r,t)T​)log(1−p(h,r,t)S​))​ 最终损失：同上L=αLsoft S+(1−α)Lhard SL=\\alpha L_{\\text {soft }}^{S}+(1-\\alpha) L_{\\text {hard }}^{S}L=αLsoft S​+(1−α)Lhard S​ 两阶段蒸馏 在前面的部分中，我们介绍了如何让学生从KGE老师那里获取知识，其中学生使用固定老师生成的硬标签和软标签进行培训。为了获得一个更好的学生，我们提出了一个两阶段的提炼方法，通过解冻老师并让老师在提炼的第二阶段向学生学习来提高学生对老师的接受度。 第一阶段 第一阶段类似于传统的知识提炼方法，在这一阶段中，教师被冻结，在培训学生时保持不变，如前一节所述。 第二阶段 在这个阶段，老师解冻，并努力调整自己，以提高对学生的接受度。基本思想是我们不仅要培训老师用一个硬标签来保证它的性能，但也要使它适合学生生成的软标签。本质上，这可以被认为是一个教师也学习转化学生的过程。因此，教师将变得更加适应学生，从而提高蒸馏效果。 For KGEs with Marginal Loss 优化老师的硬标签损失和之前一样，教师的软标签表示为： Lsoft T=∑(h,r,t)∈G(∣frS(h,t)−frT(h,t)∣+∑i=1k∣frS(hi,ti)−frT(hi,ti)∣)\\begin{aligned} L_{\\text {soft }}^{T}=\\sum_{(h, r, t) \\in G}(&amp;\\left|f_{r}^{S}(h, t)-f_{r}^{T}(h, t)\\right| \\\\ &amp;\\left.+\\sum_{i=1}^{k}\\left|f_{r}^{S}\\left(h_{i}, t_{i}\\right)-f_{r}^{T}\\left(h_{i}, t_{i}\\right)\\right|\\right) \\end{aligned} Lsoft T​=(h,r,t)∈G∑​(​∣∣​frS​(h,t)−frT​(h,t)∣∣​+i=1∑k​∣∣​frS​(hi​,ti​)−frT​(hi​,ti​)∣∣​)​ 和之前学生的软标签是相同的，因为两个数之差的绝对值具有可交换性。 For KGEs with Cross Entropy Loss. Lsoft T=−∑(h,r,t)∈G∪G−(p(h,r,t)Slog⁡p(h,r,t)T+(1−p(h,r,t)S)log⁡(1−p(h,r,t)T)).\\begin{aligned} L_{\\text {soft }}^{T}=&amp;-\\sum_{(h, r, t) \\in G \\cup G^{-}}\\left(p_{(h, r, t)}^{S} \\log p_{(h, r, t)}^{T}\\right.\\\\ &amp;\\left.+\\left(1-p_{(h, r, t)}^{S}\\right) \\log \\left(1-p_{(h, r, t)}^{T}\\right)\\right) . \\end{aligned} Lsoft T​=​−(h,r,t)∈G∪G−∑​(p(h,r,t)S​logp(h,r,t)T​+(1−p(h,r,t)S​)log(1−p(h,r,t)T​)).​ Final Loss L=αLsoft S+(1−α)Lhard S+βLsoft T+(1−β)Lhard TL=\\alpha L_{\\text {soft }}^{S}+(1-\\alpha) L_{\\text {hard }}^{S}+\\beta L_{\\text {soft }}^{T}+(1-\\beta) L_{\\text {hard }}^{T} L=αLsoft S​+(1−α)Lhard S​+βLsoft T​+(1−β)Lhard T​ Experiments 主要回答以下问题： 是否能够从老师身上提炼出一个好学生，并且比一个从无到有训练出来的同维度模型表现更好； 经过一个蒸馏程序后，推理时间提高了多少； 两阶段蒸馏方法是否以及在多大程度上有助于我们的提议 数据集和实施细节 评估指标：MR, MRR, Hit@k baselines：TransE and TransH ComplEx and RoratE Q1: Whether our method successfully distills a good student ? 我们首先训练一个有更高维度老师的学生，标记为‘DS’，然后训练一个只有硬标签丢失的同维学生，标记为‘no-DS’，这和训练同维原始KGE模型是一样的。然后我们比较它们在链路预测上的性能。表3和表4分别显示了WN18RR和FB15k-237在不同学生尺寸设置下的结果。 Higher Dimensional Teachers. 由于教师的维度可能很重要，我们还用TerE对128和256维度的教师进行了实验，以评估教师维度的影响。 结果表明:(1)对于32维的学生，高维度的教师取得的成绩稍好；(2)对于16维的学生，高维度的教师没有取得更好的成绩；(3)对于8维的学生，高维度的教师取得的成绩较差。这表明我们的方法的压缩能力大约是8倍。因此，并不总是需要从一个更大的老师那里提炼。一种直觉是，虽然更大的老师更有表现力，但过高的压缩比可能会阻止学生从老师那里吸收重要信息。 结论：如果一个要求的维度是d，那么教师的维度要求小于等于8倍的d 训练速度 Q2: Whether the distilled student successfully accelerates training and inference speed? 图3显示了有或没有蒸馏的32维学生的收敛。我们观察到，通过蒸馏，我们的方法从一开始就比“无直接序列”收敛得更快、更稳定，并最终获得更好的结果。 在第二阶段蒸馏(S2，由黑色虚线分隔的红线的右半部分)开始后，MRR略微波动，并迅速收敛到更好的结果。波动的原因是在S2之初，老师开始根据学生的软标签进行调整。 推理速度 我们重复实验3次，并报告平均时间。表5显示了时间成本以及参数数量。 结果表明，参数个数的减少与压缩率成正比，因此，与64维相比，用于保存32维、16维和8维学生的机器内存将分别节省2倍、4倍和8倍。这也表明，我们的蒸馏方法实现了几乎线性的推理加速。64维老师的推理时间大约是16维学生的5倍，接近甚至超过8维学生的10倍。 Q3: Whether and how much does the two-stage distillation approach contribute to the result? 为了研究两级蒸馏方法的影响，我们进行了一项消融实验，以比较我们的方法在去除第一级(-S1)和去除第二级(-S2)时的两级性能。表6总结了WN18RR的MRR和Hit@10结果。 去掉S1后只保留了S2(参考-S1)，整体性能低于DS。大概是因为在S2，老师和学生都会互相适应。对于一个随机初始化的学生，学生向老师传达了大部分无用的信息，这些信息可能会误导老师，使老师崩溃。 去掉S2后，只保留了S1(参考S2)，几乎所有设置的性能都会下降。以TeeE为例，与DS相比，‘S2’的32维、16维和8维学生的MRR分别下降了4%、5%和10%，表明第二阶段确实可以使师生相互适应，进一步提高成绩。 这些结果支持了我们的两阶段蒸馏的有效性，首先训练S1的学生收敛到某个性能，然后在S2共同优化教师和学生。 Conclusion and Future Work 知识图的嵌入参数过多会给实际应用场景带来巨大的存储和计算挑战。在这项工作中，我们提出了一种新的KGE蒸馏方法来压缩到低维空间。为了成功地将知识提取技术应用于特殊结构的KGE，我们针对不同的KGE设计了特定的软标签损失。为了让学生充分接受老师丰富的信息，我们的方法通过两阶段蒸馏的方法鼓励老师和学生相互适应。我们已经通过在几个不同的kge和基准数据集上的链接预测任务评估了我们的方法。实验结果表明，该方法能有效减少模型参数，大大提高推理速度，与教师相比没有太大的性能损失，推理能力也优于直接训练的同维数方法。 在这项工作中，我们只考虑了通过KGEs的最终输出来传递知识。在未来，我们将首先从其他网络探索多层蒸馏，其次，研究在更复杂的环境下，如对抗式学习和集成学习中的知识蒸馏。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://yao-chen-ecnu.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"知识蒸馏","slug":"知识蒸馏","permalink":"https://yao-chen-ecnu.github.io/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"}]},{"title":"FedGKT","slug":"FedGKT","date":"2021-06-22T02:47:54.185Z","updated":"2021-06-23T02:11:47.631Z","comments":true,"path":"2021/06/22/FedGKT/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/06/22/FedGKT/","excerpt":"","text":"Abstract 重新定义联邦学习为群组知识迁移训练算法（ group knowledge transfer，FedGKT）。设计了一种交替最小化方法的变体，在边缘节点上训练小的神经网络，并通过知识蒸馏定期将它们的知识转移到大型服务器端的神经网络。 优点：减少边缘计算、降低CNNs通信带宽、异步训练、精度不减 结合FedAvg和SL（模型并行分割学习model parallelism-based split learning），交替优化边缘模型和服务器模型。将计算负担从边缘设备转移到强大的服务器上。 使用三个不同的数据集(CIFAR-10、CIFAR-100和CINIC-10)及其非独立同分布变体来训练基于ResNet-56和ResNet-110设计的CNN。结果表明FedGKT可以获得与FedAvg相当甚至略高的精度。与使用FedAvg的边缘训练相比，FedGKT对边缘设备的计算能力要求低9到17倍，对边缘CNN的参数要求低54到105倍。我们的源代码在FedML发布(https://fedml.ai). Introduction 1、local training：边缘设备使用轻量级的CNN训练其私有数据，提取器extractor提取特征图，分类器classifier得到预测 2、periodic transfer：服务器server，输入边缘设备模型提取到的特征，输出预测，匹配边缘设备的预测（使用软标签和KD） 3、transfer back：服务器将预测回传给边缘设备，边缘设备再进行预测，匹配服务器的预测（使用软标签和KD） 4、edge-sided model：服务器是由边缘设备传递的知识训练提升，而边缘设备同样因为服务器而提升，最终模型是本地特征提取器+共享服务器模型 Group Knowledge Transfer Preliminary 目标是在许多资源受限的设备上协同训练大型卷积神经网络(如ResNet)，这些设备没有配备GPU加速器，并且不需要将每个设备的数据集集中到服务器端。 考虑在数据集上D\\mathcal{D}D对C\\mathcal{C}C个类进行监督学习，假设网络中有KKK个客户端（边缘设备），第k个节点的私有数据集可以表示为Dk:={(Xik,yi)}i=1N(k)\\mathcal{D}^{k}:=\\left\\{\\left(\\boldsymbol{X}_{i}^{k}, y_{i}\\right)\\right\\}_{i=1}^{N^{(k)}}Dk:={(Xik​,yi​)}i=1N(k)​，其中Xi\\boldsymbol{X}_{i}Xi​为第i个训练样本，yiy_iyi​是对应的标签，yi∈{1,2,…,C}y_{i} \\in\\{1,2, \\ldots, C\\}yi​∈{1,2,…,C}是个多分类任务。N(k)N^{(k)}N(k)是Dk\\mathcal{D}^{k}Dk中的样本数量。D={D1,D2,…,Dk},N=∑k=1KN(k)\\mathcal{D}=\\left\\{\\mathcal{D}_{1}, \\mathcal{D}_{2}, \\ldots, \\mathcal{D}_{k}\\right\\}, N=\\sum_{k=1}^{K} N^{(k)}D={D1​,D2​,…,Dk​},N=∑k=1K​N(k) 一般来说，我们可以将基于CNN的联合学习表述为一个分布式优化问题: w代表global CNN在每个客户端的网络权重，f(k)(W)f^{(k)}(\\boldsymbol{W})f(k)(W)代表第k个客户端的本地目标函数，用于衡量异构数据集Dk\\mathcal{D}^{k}Dk上的本地经验风险。ℓ\\ellℓ是loss function 如图2(a)所示，主要缺点是由于缺乏GPU加速器和足够的内存，这些方法无法在资源受限的边缘设备上训练大型CNN。 如图2(b)所示，并行的分割学习模型，试图通过将W分割成两部分，并将更大的部分放到服务器端来打破计算约束，但是单个小批量迭代需要远程正向传播和反向传播。对于边缘计算，如此频繁的同步机制可能会导致严重的掉队问题，从而显著降低训练过程的速度。 Reformulation Non-convex Optimization. 将WWW划分成两个部分：一个小的特征提取模型WeW_eWe​，一个大规模的服务器端模型WsW_sWs​，分别将他们放到边缘和服务器端。同时为WeW_eWe​添加一个分类器WcW_cWc​，构成一个小的可训练的模型放在客户端。我们将单个全局模型优化重新表述为一个非凸优化问题，要求我们同时求解服务器模型FsF_sFs​和边缘模型FsF_sFs​\\ell_{c}和和和\\ell_{s}分别是客户端和服务器端的损失函数，分别是客户端和服务器端的损失函数，分别是客户端和服务器端的损失函数，f_s是服务器端模型，是服务器端模型，是服务器端模型，f{(k)}$是客户端的边缘模型，包括一个特征提取器$f_{e}{(k)}和分类器和分类器和分类器f_{c}{(k)}$，$W_s$、$W_e{(k)}、、、W_c{(k)}$分别是$f_s$、$f_{e}{(k)}和和和f_{c}{(k)}$的网络权重。$H_i{(k)}是第i个样本的特征提取器是第i个样本的特征提取器是第i个样本的特征提取器f_{e}^{(k)}featuremap输出。客户端k在本地独立训练feature map输出。客户端k在本地独立训练featuremap输出。客户端k在本地独立训练f{(k)}$，服务器端使用$H_i{(k)}作为输入训练作为输入训练作为输入训练f_s$。 在推理阶段，客户端k的最终训练模型体系结构由特征提取器fe(k)f_{e}^{(k)}fe(k)​的结构和服务器模型fsf_sfs​的结构堆叠。实际上，客户端可以通过下载服务器模型fsf_sfs​并在本地使用它来运行离线推理，也可以通过与服务器的网络连接执行在线推理。 **Advantages and Challenges. ** 上述Reformulation的核心优势是，当我们假设f(k)f^{(k)}f(k)的模型规模比fsf_sfs​的模型规模小多个数量级时，边缘训练是可以承受的。此外，对于大型CNN训练，将Hi(k)H_i^{(k)}Hi(k)​传输到服务器的通信带宽大大小于传统联邦学习中通信所有模型参数的带宽。 挑战： 首先，期望每个客户机充分解决内部优化问题（等式（5））。也就是说，每个客户机都应该很好地训练其特征提取器fe(k)f_{e}^{(k)}fe(k)​，以确保等式（3）能够准确地为任何给定的输入图像生成Hi(k)H_i^{(k)}Hi(k)​。然而，在FL设置中，每个边缘设备上的数据集很小，因此可能不足以训练仅基于本地数据集的基于CNN的特征提取器。此外，外部优化公式（2）和内部优化公式（5）相互关联：公式（2）依赖于由公式（5）优化的Hi(k)H_i^{(k)}Hi(k)​的质量。如果单个客户端特征提取器fe(k)f_{e}^{(k)}fe(k)​没有得到充分训练，则这种相关性进一步使得外部优化等式（2）难以收敛。 Group Knowledge Transfer (FedGKT) 鉴于上述挑战，我们将知识蒸馏损失纳入优化方程以规避优化困难。 直觉是，从服务器模型转移的知识可以促进边缘优化（等式（5））。因此，我们建议双向转移群体知识。服务器CNN从多个边缘吸收知识，单个边缘CNN从服务器CNN获得增强的知识。更具体地说，在式（2）和式（5）中，我们设计了如下损失函数： ℓCE\\ell_{CE}ℓCE​是预测值与真实标签的加插上损失。DKLD_{KL}DKL​是KL散度。 pki=exp⁡(zc(k,i)/T)∑i=1Cexp⁡(zc(k,i)/T)\\boldsymbol{p}_{k}^{i}=\\frac{\\exp \\left(z_{c}^{(k, i)} / T\\right)}{\\sum_{i=1}^{C} \\exp \\left(z_{c}^{(k, i)} / T\\right)}pki​=∑i=1C​exp(zc(k,i)​/T)exp(zc(k,i)​/T)​ ，psi=exp⁡(zsi/T)∑i=1Cexp⁡(zsi/T)\\boldsymbol{p}_{s}^{i}=\\frac{\\exp \\left(z_{s}^{i} / T\\right)}{\\sum_{i=1}^{C} \\exp \\left(z_{s}^{i} / T\\right)}psi​=∑i=1C​exp(zsi​/T)exp(zsi​/T)​ 它们分别是边缘模型f(k)f^{(k)}f(k)和服务器模型fsf_sfs​的概率预测。对logitzzz计算softmax值。logitzsz_szs​和zc(k)z_c^{(k)}zc(k)​分别是服务器模型和客户机模型中最后一个全连接层的输出。TTT是softmax函数的蒸馏温度超参数。 这样，服务器模型吸收了从每个边缘模型获得的知识。类似地，边缘模型试图使其预测更接近服务器模型的预测，从而吸收服务器模型知识以提高其特征提取能力。 Improved Alternating Minimization 在将式（6）和（7）带入我们的重新公式（式（2）和（5））之后，我们提出了一种交替最小化（AM）的变体来解决重新公式化的优化问题，如下所示： 在式（8）中，我们为几个epoch固定W(k)W^{(k)}W(k)并优化（训练）WsW_sWs​，然后我们切换到（10）为几个epoch固定WsW_sWs​并优化W(k)W^{(k)}W(k)。这种优化在等式（8）和（10）之间的进行多轮，直到达到收敛状态。 Training Algorithm 在每一轮训练中，客户机使用本地SGD训练多个epoch，然后将提取的feature map和logit信息发送给服务器。当服务器从每个客户机接收提取的feature map和logit信息，用于训练更大的服务器端CNN。然后服务器将其全局logit信息发送回每个客户机。这个过程在多个回合中迭代，在每一回合中，所有客户机的知识都被传递到服务器模型，反之亦然。为了评估FedGKT的有效性，我们设计了基于ResNet[1]的CNN架构，如图1（b）所示。 Experiments Experimental Setup Implementation 我们基于FedML开发了FedGKT训练框架，FedML是一个开源的联邦学习研究库，简化了新算法的开发，并将其部署在分布式计算环境中。我们的服务器节点有4个NVIDIA RTX 2080Ti GPU，有足够的GPU内存用于大型模型训练。我们使用几个基于CPU的节点作为客户机来训练小型CNN。 Task and Dataset 我们的训练任务是在CIFAR-10、CIFAR-100和CINIC-10上进行图像分类。我们还通过将训练样本分成K个不平衡分区使得客户端的数据为非独立同分布。 Baselines 我们将FedGKT与最新的FL方法FedAvg和集中训练方法进行了比较。基于分裂学习的方法用于比较通信成本。 Model Architectures 评估了两种现代CNN架构：ResNet-56和ResNet-110。baseline FedAvg要求所有边缘节点使用这两个CNN进行训练。对于FedGKT，设计了基于这两个CNN的边缘和服务器端模型。在边缘，我们设计了一个称为ResNet-8的小型CNN结构，包含8个卷积层。服务器端模型体系结构是ResNet-55和ResNet-109，它们具有相同的输入维度，以匹配边缘特征提取器的输出。对于分割学习，我们使用ResNet-8中的提取器作为CNN的边缘部分，而CNN的服务器端部分也是ResNet-55和ResNet-109。 Result of Model Accuracy 对于标准实验，我们在16个客户机和一个GPU服务器上运行所有数据集和模型。图3显示了在3个数据集的ResNet-56模型上训练时的测试精度曲线。它包括集中培训、FedAvg和FedGKT的结果。我们还将ResNet-56和ResNet-110的所有数值结果总结在表1中。在I.I.D.和非I.I.D.设置中，FedGKT获得了与FedAvg相当甚至更好的精度。 Efficiency Evaluation 为了比较训练的计算需求，我们使用先前的方法计算了在边缘执行的浮点运算的次数[70,71]。我们在CIFAR-100上的结果如图4所示。与FedAvg基线相比，我们的FedGKT（ResNet-8）边缘的计算成本是ResNet-56的1/9，是ResNet-110的17/1（内存成本比较可以通过模型参数数粗略比较：ResNet-8有11K个参数，它比ResNet-56少54倍，比ResNet-110少105倍。我们还测试了inteli7 cpu上每小批（批大小为64）的CPU运行时间（它比现有的边缘设备有更高的性能）。结果表明，ResNet-8只需要ResNet-110训练时间的3%（30ms vs.950ms）。 为了比较通信成本，我们使用SL（分割学习）作为基线，它还交换隐藏的特征映射，而不是整个模型。在不使用数据压缩技术的情况下，计算通信成本。结果如图5所示（X轴单位：GB）。FedGKT与服务器之间的特征映射交换比SL少。 不同条件下FedGKT的消融实验 The Effectiveness of Knowledge Transfer 我们观察到，从服务器到边缘的传输总是有帮助的，而随着数据集变得越来越困难，双向传输（S&lt;–&gt;E）更加有效（CIFAR-100）。 Asynchronous Training 由于服务器不需要等待来自所有客户机的更新来开始训练，FedGKT自然支持异步训练。实验结果如表3所示。结果表明，异步训练对模型精度没有负面影响。这说明了我们的方法比SL的优势，在SL中，每个边都需要对每个小批量迭代进行多次同步。 FedGKT with Different Edge Number 为了了解FedGKT的可扩展性，我们评估了它在不同边缘节点下的性能。测试精度结果如表4所示。一般来说，增加更多的边缘节点不会对精度产生负面影响。 Smaller Architectures 我们使用更小的边缘模型测试了FedGKT的性能：CIFAR-10上的ResNet-4和ResNet-6。ResNet-4和ResNet-6分别使用一个和两个基本块组件（包括两个卷积层）。结果如表5所示。虽然将边缘模型尺寸减小到ResNet-8并没有降低精度，但当模型尺寸减小得更大时，它确实会降低整体精度。 Discussion 方法的局限性 隐私安全：但基于以前的差分隐私和多方计算，交换隐藏的特征图比交换模型或梯度更安全。在训练阶段隐藏映射交换。模型或梯度交换可能泄露隐私，缺乏分析 通信成本：隐藏向量比权重或梯度小，每个数据点的隐藏向量可以独立传输，FedGKT比梯度或模型交换要求更小的带宽，通信代价取决于数据点的数量，在样本数量非常大，图像分辨率非常高的情况下，方法和分割学习总的来说都有很高的通信成本。 标签缺陷：只能用于监督学习，缺乏足够的标签 可伸缩性：在跨设备设置中，我们需要使用大量的智能手机协同训练模型(如客户端数量高达100万)。 Conclusion 在这项工作中，为了解决资源受限的现实，我们将FL重新构造为一个群体知识转移（FedGKT）训练算法。FedGKT可以有效地训练边缘上的小CNN，并通过知识提取将其知识周期性地传递给大容量的服务器端CNN。 FedGKT在一个框架中实现了几个优点：减少了对边缘计算的需求，降低了大型cnn的通信成本，以及异步训练，同时保持了与FL相当的模型精度。 为了简化边缘训练，我们还开发了一个基于FedGKT的分布式训练系统。我们通过在三个不同的数据集（CIFAR-10、CIFAR-100和CINIC-10）及其非独立同分布变体上训练CNN架构（ResNet-56和ResNet-110）来评估FedGKT。我们的结果表明，FedGKT可以获得相当甚至略高的精度。与使用FedAvg的边缘训练相比，FedGKT的计算能力（FLOPs）降低了9到17倍，所需的参数也减少了54到105倍。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://yao-chen-ecnu.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"联邦学习","slug":"联邦学习","permalink":"https://yao-chen-ecnu.github.io/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"FitNets","slug":"FITNETS","date":"2021-06-22T02:47:54.180Z","updated":"2021-06-23T01:39:21.956Z","comments":true,"path":"2021/06/22/FITNETS/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/06/22/FITNETS/","excerpt":"","text":"Abstract 论文主要针对Hinton提出的知识蒸馏法进行扩展，允许Student网络可以比Teacher网络更深更窄，使用teacher网络的输出和中间层的特征作为提示，改进训练过程和student网络的性能。CIFAR10上的深的student网络参数比先进teacher网络的少10.4倍。 Introduction 所有之前的工作都是专注于压缩teacher网络到更浅更宽的网络，没有充分利用深度。深度是特征学习的基本层面，鼓励特征重用。 论文通过利用深度解决网络压缩的问题，提出新方法训练thin and deep网络FitNets。 方法是对Hinton的知识蒸馏进行的扩展。 Method 老师网络是一个wide网络，学生网络是thin and deep网络 具体步骤如下： 一、选择teacher模型特征提取器的第NNN层输出作为hint，从第一层到第NNN层的参数对应图(a)中的WHintW_{Hint}WHint​ 二、选择student模型特征提取器的第MMM层输出作为guided，从第一层到第MMM层的参数对应图(a)中的WGuidedW_{Guided}WGuided​ 三、步骤一与步骤二的特征图维度可能不匹配，因此引入卷积层调整器，记为rrr，对guided的维度进行调整 四、进入阶段一训练，最小化如下损失函数： uhu_huh​表示teacher模型从第一层到第NNN层对应的函数，ugu_gug​表示student模型从第一层到第MMM层对应的函数，rrr表示卷积层调整器，对应的参数记为WrW_rWr​ 五、因为阶段一没有label信息，蒸馏粒度不够细，因此论文引入阶段二的训练，利用hinton提出的KD对student模型进行蒸馏，如图©，将小网络学习到的guided层参数作为初始权重，然后按照正常的蒸馏学习方法，跟大网络学习soft labels，损失函数如下： 整体训练过程如下图算法所示： Experiments 在四个数据集上进行实验 实验结果上，student比teacher参数少，效果比teacher还要好","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://yao-chen-ecnu.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"知识蒸馏","slug":"知识蒸馏","permalink":"https://yao-chen-ecnu.github.io/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"}]},{"title":"A gift from KD","slug":"A Gift from Knowledge Distillation","date":"2021-06-22T02:47:54.176Z","updated":"2021-06-23T01:44:52.592Z","comments":true,"path":"2021/06/22/A Gift from Knowledge Distillation/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/06/22/A%20Gift%20from%20Knowledge%20Distillation/","excerpt":"","text":"1、Knowledge Distillation 先是 Hinton 提出来的，大意是说：小模型在分类的时候，光用训练集里的 one-hot label 不好，因为这种标注把类别间的关系割裂开了。而如果让小模型跟着大模型输出的概率分布去学的话，就相当于给出了类别之间的相似性信息，提供了额外的监督信号，因此学起来更容易。比如说识别手写数字，同时标签为 3 的图片，可能有的比较像 8，有的比较像 2，one-hot labels 区分不出来这个信息，但是一个 well-trained 大模型可以给出。因此，修改一下损失函数，让小模型在拟合训练数据的 ground truth labels 的同时，也要拟合大模型输出的概率分布。这个方法叫做 KD Training (Knowledge Distillation Training)。 2、然后 Romero 又提出了 FitNet，大意是说：直接让小模型在输出端模仿大模型，这个对于小模型来说太难了（模型越深越难训，最后一层的监督信号要传到前面去还是挺累的）。解决方案就是说，不如在中间加一些监督信号。于是训练的方法就是所谓的 hint training，把网络中间的输出也拿出来，让大模型和小模型中间层的输出也要尽量接近，让小模型去学习大模型做预测时的中间步骤。 实际做的时候是用**两阶段法：先用 hint training 去 pretrain 小模型前半部分的参数，再用 KD Training 去训练全体参数。**这感觉就好像是，我们的目的是让学生做高考题，那么就先把初中的题目给他教会了（先让小模型用前半个模型学会提取图像底层特征），然后再回到本来的目的、去学高考题（用 KD 调整小模型的全部参数）。 3、之前的文章都是把大模型的输出当成小模型要去拟合的目标，后来又有文章提出了新的方法，不拟合大模型的输出，而是去拟合大模型层与层之间的关系，这才是转移和蒸馏的知识。这个关系是用层与层之间的内积来定义的。 Abstract 提出了将蒸馏的知识看作成一种解决问题的流，它是在不同层之间的feature通过内积计算得到的 这个方法有三个好处： student网络可以学的更快 student网络可以超过teacher网路的性能 可以适用于迁移学习（teacher和student属于不同的任务） Introduction 将知识看作如何解决问题的流，所以将要蒸馏知识看作解决问题的流 流被定义为在两个不同层上的features上的关系 Gram matrix是通过计算特征间的内积得到的，可以表示输入图像的纹理信息，本文也是通过计算Gram matrix来得到流，不同点在于原本的Gram matrix是计算一个层的特征间的内积，而本文是结算不同层特征间的内积。 主要贡献： 提出了一种好的知识蒸馏的方法 这种方法对快速优化有帮助 这种方法可以显著提升student网络的性能 这种方法适用于迁移学习 Method 提出观点 将输入和输出看作是问题和答案，中间层看作是解决问题的一个步骤，按照Fitnets的思路，会学习中间这个步骤，然而解决这个问题可以有很多路径，中间这个步骤的状态不一定是一种，所以Fitnets给了太多的限制，作者提出学习输入和输出的关系，而不是直接学习中间的步骤的状态 数学表达式 表达式 Gi,j(x;W)=∑s=1h∑t=1wFs,t,i1(x;W)×Fs,t,j2(x;W)h×wG_{i, j}(x ; W)=\\sum_{s=1}^{h} \\sum_{t=1}^{w} \\frac{F_{s, t, i}^{1}(x ; W) \\times F_{s, t, j}^{2}(x ; W)}{h \\times w} Gi,j​(x;W)=s=1∑h​t=1∑w​h×wFs,t,i1​(x;W)×Fs,t,j2​(x;W)​ FSP矩阵G∈Rm×nG \\in \\mathbb{R}^{m \\times n}G∈Rm×n由两层输出的特征来生成，其中一层的feature map为F1∈Rh×w×mF^{1} \\in \\mathbb{R}^{h \\times w \\times m}F1∈Rh×w×m，另外一层的feature map为$$F^{2} \\in \\mathbb{R}^{h \\times w \\times n}$$ 其中i，ji，ji，j表示F1与F2的通道号，xxx和WWW分别代表输入图像和DNN的权重 这个式子其实就是不同通道的特征的相互内积 计算G的位置 FSP Matrix的损失 LFSP(Wt,Ws)=1N∑x∑i=1nλi×∥(GiT(x;Wt)−GiS(x;Ws)∥22\\begin{array}{l} L_{F S P}\\left(W_{t}, W_{s}\\right) \\\\ =\\frac{1}{N} \\sum_{x} \\sum_{i=1}^{n} \\lambda_{i} \\times \\|\\left(G_{i}^{T}\\left(x ; W_{t}\\right)-G_{i}^{S}\\left(x ; W_{s}\\right) \\|_{2}^{2}\\right. \\end{array} LFSP​(Wt​,Ws​)=N1​∑x​∑i=1n​λi​×∥(GiT​(x;Wt​)−GiS​(x;Ws​)∥22​​ n代表在student中选择的层对数 N表示样本数量 T，S分别代表teacher和student 学习步骤 首先，对教师网络进行数据预训练。此数据集可以与学生网络将学习的数据集相同，也可以不同。对于迁移学习任务，教师网络使用不同于学生网络的数据集。其次，教师网络与学生网络相同或更深。 Experiment 实验采用图2中的resnet，对于FSP矩阵的生成，选择每个残差块中的第一层和最后一层的feature map来计算。由于FSP矩阵需要两个feature map的大小相同，因此，在两层特征不同的情况下，引入最大值池化层进行处理，得到两个相同大小的feature map。 baseline：Fitnet Fast optimization 本文提出的快速优化技术的目标是通过使用比正常训练程序更少的训练时间，使学生网络集成达到与教师网络集成相似的性能。 在两个数据集上进行训练 CIFAR-10、 CIFAR-100 使用26层残差网络作为教师网络，得到92%的准确度。本实验中学生网络与教师网络的层数相同。 图3表示测试精度和训练损失随时间的变化。学生网络的优化速度快于教师网络。学生网络收敛的速度是教师网络的三倍。由于我们对教师网络采用MSRA初始化技术，这不是一种简单的初始化方法，而是一种高性能的初始化方法，因此我们认为FSP矩阵为初始化学生网络的权值提供了很好的提取知识。 关于Student与FitNet的分析 由于本文提出的结构是学习一种输入和输出的关系，本文是通过FSP实现的，所以多个FSP之间可以相对独立一些，整个模块可以解耦； 而对于FitNet，假设加入三个中间层，在第二个中间层和第三个中间层不好去学习，因为要想学习好他们，首先要保证前边的一层中间层学习好，所以FitNet这种直接用特征做监督信息的方式不能解耦多个loss层，这也是为什么三层FitNet没有一层FitNet效果好的原因； FSP相比FitNet，赋予了网络更大的自由。如果student与teacher网络有相同的中间层，那么肯定有相同的FSP，但反过来确不成立，FSP的相同并不限制中间层的具体状态。 关于加强多个Student的不相关性，从而提升集成模型的准确率 Table1的倒数第二行：虽然student网络的单体能力已经超过了teacher网络，但是集成的student网络确没有集成的teacher的集成效果好，这是因为多个student网络的FSP矩阵是一致的，导致他们的相关性太大 Table1的倒数第一行：作者提出了将生成的FSP矩阵的行和列进行重新洗牌，得到新的几个FSP，用新的FSP训练student得到的集成效果要好。其实这相当于将生成FSP的两个不同层的特征的通道打乱而得出的FSP，本质上没有改变信息的内容。 就训练次数而言，原模型的迭代次数为16 s/100 iterations，而第1阶段的训练次数为35 s/100 iterations。因此，就总学习时间而言，用原方法训练3名教师dnn用8.6h，用本方法训练3名学生dnn用4.84h。后者是后者的1.78倍。 Performance improvement for the small DNN Transfer Learning Teacher-fine tuning是在34层的网络上进行迁移学习得到的结果，Proposed Method是在20层的网络上进行FSP学习得到的结果，可以看出，已经很接近了。 Conclusion 提出了一种从DNN中提取知识的新方法。通过将提取的知识确定为用所提出的FSP矩阵计算的求解过程的流程，该方法优于现有的知识迁移方法。我们从三个重要方面验证了所提方法的有效性。该方法优化DNN的速度更快，性能更高。此外，该方法还可用于迁移学习任务。 打个比方：如果做一道平面几何题，题目要证两线段相等，做法是先要做一条辅助线，再证三角形 X 和 Y 全等，最后证明原题结论。那么前面 Romero 的方法就是，中间每一步都盯着你做，你必须和我做的一样，我教你这道题的辅助线该怎么连，然后证哪两个三角形全等；而本文的方法是说，你要先学会『辅助线-&gt;三角形全等-&gt;线段相等』这种做题的套路，而不是去学一个具体的题目里辅助线到底怎么连，我把一般的解题方法教给你，具体每一题每一步到底怎么证你自己琢磨琢磨就会了。最后训练的时候同样也是用二阶段法：先根据大模型的 FSP 矩阵调整小模型参数，使得小模型层间关系也和大模型的层间关系类似；然后直接用原损失函数（如交叉熵）继续精调小模型参数。感觉这种学习方法比较高屋建瓴，用人类学知识的过程来比喻的话，就是先学抽象的道，培养对于问题的宏观认识，培养整体观念，了解这个问题分为需要几大块，各个块之间的关系是什么样的；然后再学具体的术，填充完善知识细节。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://yao-chen-ecnu.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"知识蒸馏","slug":"知识蒸馏","permalink":"https://yao-chen-ecnu.github.io/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"}]},{"title":"FedMD","slug":"FedMD-Heterogenous Federated Learning","date":"2021-06-22T02:47:54.170Z","updated":"2021-06-23T01:28:09.869Z","comments":true,"path":"2021/06/22/FedMD-Heterogenous Federated Learning/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/06/22/FedMD-Heterogenous%20Federated%20Learning/","excerpt":"","text":"Abstract 联邦学习能够创建一个强大的集中式模型，而不会损害多个参与者的数据隐私。虽然成功了，但它没有包含每个参与者独立设计自己的模型的情况。由于知识产权问题以及任务和数据的异构性，这是联合学习应用于医疗保健和人工智能即服务等领域的普遍要求。在这项工作中，我们使用迁移学习和知识蒸馏来开发一个通用框架，当每个代理不仅拥有他们的私有数据，而且拥有独特设计的模型时，该框架能够实现联合学习。我们在MNIST/FEMNIST数据集和CIFAR10/CIFAR100数据集上测试了我们的框架，并观察到所有参与模型的快速改进。有10个不同的参与者，每个模型的最终测试精度平均比没有协作的情况下高20%，仅比每个模型在所有私有数据集被汇集并直接提供给所有参与者的情况下获得的性能低几个百分点。 Introduction 联合学习面临许多挑战[5]，其中特别重要的是学习过程各个方面出现的异构性。当每个参与者具有不同的带宽和计算能力时，就存在系统异构性；联邦学习的本地异步方案部分解决了这一问题，该方案得到了进一步完善，例如支持主动采样[6，7]和提高容错能力[8]。还有统计异质性(非i.i.d .问题)，客户有不同数量的数据来自不同的分布[9，10，11，12，13，14]。 在这项工作中，我们关注不同类型的异质性:局部模型的差异。在最初的联邦框架中，所有用户都必须同意集中式模型的特定架构。当参与者是数以百万计的手机等低容量设备时，这是一个合理的假设。在这项工作中，我们转而探索联邦框架的扩展，这在面向业务的环境中是现实的，其中每个参与者都有能力和愿望来设计他们独特的模型。这出现在医疗保健、金融、供应链和人工智能服务等领域。例如，当几个医疗机构在不共享私有数据的情况下进行合作时，他们可能需要创建自己的模型来满足不同的规范。出于隐私和知识产权的考虑，他们可能不愿意分享他们模型的细节。再比如AI即服务。一个典型的人工智能供应商，例如客户服务聊天机器人，可能有几十个客户公司。每个客户的模型是不同的，并解决不同的任务。标准的做法是只使用客户自己的数据来训练客户的模型。如果能够在不损害隐私或独立性的情况下利用来自其他客户端的数据，将会非常有益。当每个参与者都有一个对其他人来说是黑箱的不同模型时，如何进行联合学习？这是我们在这部作品中要回答的中心问题。 全模型异构的关键是沟通。特别是，必须有一个翻译协议，使深层网络能够理解他人的知识，而无需共享数据或模型架构。这个问题涉及深度学习的基本问题，例如可解释性和紧急通信协议。原则上，机器应该能够学习适应任何特定用例的最佳通信协议。作为这个方向的第一步，我们采用一个基于知识提炼的更透明的框架来解决这个问题。 迁移学习是解决私人数据稀缺的另一个主要框架。在这项工作中，我们的私有数据集可以小到每个类几个样本。因此，除了联合学习之外，使用来自大型公共数据集的迁移学习也是势在必行的。我们通过两种方式利用迁移学习的力量。首先，在进入协作之前，每个模型首先在公共数据上，然后在它自己的私有数据上被完全训练。第二，也是更重要的，黑盒模型基于它们在公共数据集样本上的输出类分数进行通信。这是通过知识提炼[15]实现的，知识提炼能够以模型不可知的方式传输学习到的信息。 贡献:这项工作的主要贡献是FedMD，这是一个新的联邦学习框架，使参与者能够独立设计他们的模型。我们的集中式服务器不控制这些模型的架构，只需要有限的黑盒访问。我们确定这个框架的关键要素是在参与者之间翻译知识的通信模块。我们通过利用转移学习和知识提炼的力量来实现这样的通信协议。 Methods Problem definition 在联邦学习的过程假设有mmm个参与者，每个参与者拥有一个非常小的标记数据集Dk:={(xik,yi)}i=1Nk\\mathcal{D}_{k}:=\\left\\{\\left(x_{i}^{k}, y_{i}\\right)\\right\\}_{i=1}^{N_{k}}Dk​:={(xik​,yi​)}i=1Nk​​，并且它们可能不是来自于同一个分布。 另外存在一个大的公共数据集D0:={(xi0,yi0)}i=1N0\\mathcal{D}_{0}:=\\left\\{\\left(x_{i}^{0}, y_{i}^{0}\\right)\\right\\}_{i=1}^{N_{0}}D0​:={(xi0​,yi0​)}i=1N0​​，每个参与者都能从中获取数据。 每个参与方运行一个独立的模型fkf_kfk​，可以是结构不同的模型，模型的超参也不需要在参与方之间进行共享。 目标是建立一个协作框架，通过本地数据集Dk\\mathcal{D}_{k}Dk​和公共数据集D0\\mathcal{D}_{0}D0​来提升每个参与方单独训练fkf_kfk​的模型性能。 异构联合学习的通用框架。每个代理（参与方）都拥有一个私有数据集和一个独立设计的模型。为了在没有数据泄露的情况下进行交流和协作，代理需要将他们所学的知识转换成标准格式。中央服务器收集这些知识，计算分布在网络上的共识。在这项工作中，translator是利用知识蒸馏实现的。 The framework for heterogeneous federated learning 迁移学习。每个客户端先依次用公共数据集D0\\mathcal{D}_{0}D0​和自己本地的数据集Dk\\mathcal{D}_{k}Dk​来训练自己的模型。 重复下面五步 第一步：每个客户端用自己的本地模型fkf_{k}fk​来预测共享数据集并将预测结果分数fk(xi0)f_{k}\\left(x_{i}^{0}\\right)fk​(xi0​)发给服务端。需要注意的是 并不需要预测全部共享数据集，只需要随意选取一部分。原因是这样可以在不损害性能的前提下加快速度； 这里的预测预测结果分数是指不经过softmax的结果。 第二步：服务中心将客户端传送的分类分数取平均，得到平均分数f~(xi0)\\tilde{f}\\left(x_{i}^{0}\\right)f~​(xi0​)，即得到各个模型的一个全局共识。需要注意的是权重是可以修改的。CIFAR中，作者稍微抑制了来自两个较弱模型（0和9）的贡献。当有非常不同的模型或数据时，这些权重可能变得更重要。 第三步：每个客户端从服务器下载平均分数f~(xi0)\\tilde{f}\\left(x_{i}^{0}\\right)f~​(xi0​) 第四步： 模型蒸馏。 每个客户端用模型蒸馏在共享数据集去拟合这个平均分数，即各个模型去学习全局共识。 第五步：每个客户端在本地数据集上训练模型几个epoch。 Results 我们在两个不同的环境中测试这个框架。在第一种环境中，公共数据是MNIST，私有数据是FEMNIST的一个子集。我们考虑了IID的情况，其中每个私有数据集都是从FEMNIST中随机抽取的，以及非IID的情况，其中每个参与者虽然在培训期间只被给予一个作者写的字母，但在测试时被要求对所有作者写的字母进行分类。 在第二个环境中，公共数据集是CIFAR10，私有数据集是CIFAR100的子集，CIFAR 100有100个子类，属于20个超类，例如熊、豹、狮子、老虎和狼属于大型食肉动物。在IID案例中，任务是让每个参与者将测试图像分成正确的子类。非IID案例更具挑战性:在培训期间，每个参与者都有来自每个超类的一个子类的数据；在测试时，参与者需要将通用测试数据分类到正确的超类中。例如，一个只在训练中见过狼的参与者被期望将狮子正确地归类为大型食肉动物。因此，它必须依靠其他参与者传达的信息。 在每个环境中，10名参与者设计独特的卷积网络，这些网络可能因通道数量和层数而异，详见表1、2。首先，它们在公共数据集上进行训练，直到收敛——这些模型在MNIST的测试准确率通常在99%左右，在CIFAR10的测试准确率为76%。其次，每个参与者在自己的小型私有数据集上训练自己的模型。在这些步骤之后，他们经历了合作培训阶段，在此期间，模型获得了强大和快速的全面改进，并迅速超过了迁移学习的基线。我们使用Adam优化器[18]，初始学习率为0.001；在每一轮合作训练中，我们随机选择大小为5000的子集dj⊂D0d_j\\subset\\mathcal{D}_{0}dj​⊂D0​作为交流的基础。 FedMD提高了参与模型超出基线的测试精度。虚线(左边)表示用公共数据集和自己的小型私有数据集进行完全转移学习后模型的测试精度。这条基线是我们的起点，与相应学习曲线的起点重叠。如果来自所有参与者的私有数据集被解密并提供给组中的每个参与者，则虚线(右侧)表示模型的潜在性能。 Discussion and conclusion 在这项工作中，我们提出了FedMD，这是一个支持独立设计模型的联合学习的框架。我们的框架是基于知识提炼的，并且经过测试可以在各种任务和数据集上工作。在未来，我们将探索更复杂的通信模块，如功能转换和紧急通信协议，这将进一步提高我们框架的性能。我们的框架也可以应用于涉及自然语言处理和强化学习的任务。我们将把我们的框架扩展到极端的异构情况，包括数据量、模型容量和非常不同的本地任务之间的巨大差异。我们相信，在面向深度学习应用的广泛业务领域，异构联合学习将是未来的一个重要工具。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://yao-chen-ecnu.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"联邦学习","slug":"联邦学习","permalink":"https://yao-chen-ecnu.github.io/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"Federated Distillation and Augmentation under Non-IID Private Data","slug":"Communication-Efficient On-Device Machine Learning","date":"2021-05-13T09:19:52.685Z","updated":"2021-06-23T01:34:08.060Z","comments":true,"path":"2021/05/13/Communication-Efficient On-Device Machine Learning/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/05/13/Communication-Efficient%20On-Device%20Machine%20Learning/","excerpt":"","text":"Abstract 设备上的机器学习(ML)使训练过程能够利用大量用户生成的私有数据样本。为了享受这一好处，设备间的通信开销应该最小化。为此，我们提出了一种分布式模型训练算法——联合蒸馏(FD)，它的通信有效载荷比基准方案——联邦学习(FL)小得多，特别是当模型较大时。此外，不同设备的用户的数据样本很可能是No-IID，这通常会降低与IID数据集相比的性能。为了解决这个问题，我们提出了联合增强(FAug)，其中每个设备共同训练一个生成模型，从而增强其本地数据从而获得IID数据集。实证研究表明，与FL相比，FD+FAug的通信开销减少了约26倍，同时测试精度达到95-98%。 Introduction 在FL中，在每个设备端执行训练过程需要与模型size成比例的通信开销，从而无法使用很大的模型。此外，用户的训练数据集在不同设备之间可能是No-IID的。与IID数据相比，与IID数据集相比，No-IID 使得MNIST和CIFAR-10的预测精度分别降低了11%和51%。降低的准确性可以通过交换数据样本来部分恢复，但是这可能会导致过多的通信开销和隐私泄露。 因此，提出一种communication-efficient on-device ML approach under non-IID private data。为了提高通信效率，我们提出了一种分布式在线知识提取方法——联合提取（FD），其通信负载大小不依赖于模型大小，而是依赖于输出维度。在运行联邦蒸馏之前，我们通过联邦增强（FAug）来矫正No-IID训练数据集，这是一种使用GAN的数据增强方案，该网络是在隐私泄露和通信开销之间的权衡下集体训练的。经过训练的GAN使每个设备能够本地复制所有设备的数据样本，从而使训练数据集变的IID。 Federated distillation 传统的分布式训练算法每个epoch都交换局部模型参数。在移动设备无线互连的设备上移动通信中，这导致了显著的通信开销。FL通过隔一段时间交换模型参数来降低通信成本[3–9]。在这种周期性通信的基础上，提出的FD不交换模型参数，而是交换模型输出，允许设备上的ML采用size较大的本地模型。 FD的基本操作程序遵循知识蒸馏的在线版本[10]，也称为共蒸馏(CD)。在CD中，每个设备都把自己当成学生，把所有其他设备的平均模型输出视为老师的输出。每个模型输出是一组通过softmax函数归一化的logit值，称为logit向量，它的大小由标签的数量给出。使用交叉熵周期性地测量师生输出差异，该交叉熵成为学生的损失正则化因子，称为蒸馏正则化因子，从而在分布式训练过程中获得其他设备的知识。 然而，CD远非通信高效。原因是每个logit向量都与其输入的训练数据样本相关联。因此，为了进行知识提炼，教师和学生的输出都应该使用相同的训练数据样本进行评估。这不允许周期性的模型输出交换。相反，它需要交换与训练数据集大小一样多的模型输出，或者模型参数，以便再现的教师模型可以与学生模型同步地本地生成输出。 为了纠正这一点，FD中的每个设备存储每个标签的平均logit向量，并定期将这些局部平均logit向量上传到服务器。对于每个标签，来自所有设备的上传的局部平均logit向量被平均，得到每个标签的全局平均logit向量。所有标签的全局平均logit向量被下载到每个设备。然后，每个设备根据本地数据的标签，选择对应的教师模型的标签计算正则化项。 集合S表示所有设备的整个训练数据集，B表示每个设备的批次。函数F(w，a)是由softmax函数归一化的logit向量，其中w和a是模型的权重和输入。函数φ(p，q)是p和q之间的交叉熵，用于损失函数和蒸馏正则化。项η是常数学习率，γ是蒸馏正则化的权重参数。 FD：生成模型，保存本地输出–&gt;将本地输出的平均（加权平均）和标签上传至服务器–&gt;聚合后下载全局平均和标签–&gt;正则化局部模型 Federated augmentation 我们提出FAug，其中每个设备可以使用生成模型生成本地生成缺失的数据样本。 生成模型是在具有高计算能力和快速互联网连接的服务器上训练的。FAug中的每个设备识别数据样本中缺少的标签，称为目标标签，并通过无线链路将这些目标标签的少量种子数据样本上传到服务器。服务器对上传的种子数据样本进行过采样。最后，下载经过训练的GAN生成器使每个设备能够补充目标标签，直到训练数据集达到IID，这与直接数据样本交换相比显著降低了通信开销。这个过程如图1(b)所示 FAug的运行需要保证用户生成数据的隐私性。事实上，每个设备的数据生成偏差，即目标标签，可以容易地揭示其隐私敏感信息，例如，揭示诊断结果的患者体检项目。为了保持这些目标标签相对于服务器的私密性，设备还从目标标签之外的标签上传冗余数据样本。因此，以额外的上行链路通信开销为代价，减少了从每个设备到服务器的隐私泄露，称为设备-服务器隐私泄露。 在第I个设备上，它的设备-服务器PL被测量为 只要设备的数量足够大，就可以实现最小泄漏，而不管目标和冗余标签的大小如何。 Evaluation 在本节中，我们在No-IID MNIST训练数据集下评估FD+FAug，该数据集由以下程序构建。在具有55000个样本的MNIST训练数据集中，我们统一随机选择2000个样本，并将它们分配给每个设备。这2000个样本的集合根据标签分成10个子集。然后，在每个设备上，我们均匀地随机选择目标标签，并消除目标标签中大约97.5%的样本，使得每个目标标签包含5个样本。 每个设备都有一个5层卷积神经网络(CNN)，由2个卷积层、1个最大池层和2个全连接层组成。该设备进行本地训练，批量设置为64。本地迭代250次进行交换，全局迭代16次。 a说明了当“2”为目标标签时每个标签的测试准确性。对于目标标签，参考设备在原始非IID数据集下的独立训练产生了3.585%的测试精度，通过FAug结合FD或FL提高了73.44%或92.19%，验证了FAug结合FD和FL的有效性。 b与独立训练相比，使用FL+FAug实现了大约2倍的测试精度 c描述了设备-服务器PL随着冗余标签数量的增加而减少，随着目标标签数量的增加而增加。 Concluding remarks 实证研究表明，与FL相比，FD+FAug的有效性以小得多的通信开销实现了相对较高的准确性。 在以后的工作中，只要稍加修改，FD的性能还可以进一步提高。 此外，FD和FL可以结合起来，以平衡通信效率和准确性。例如，可以利用上行链路中的频差和下行链路中的频差，理由是下行链路无线通信链路通常比上行链路快[13]。 最后，利用差分隐私框架[8]，通过在上传的种子数据样本中插入适量的噪声，可以改善FAug的隐私保障，这是未来研究的一个有趣课题。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://yao-chen-ecnu.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"联邦学习","slug":"联邦学习","permalink":"https://yao-chen-ecnu.github.io/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"知识蒸馏","slug":"知识蒸馏","date":"2021-04-21T02:54:58.228Z","updated":"2021-04-22T07:10:42.155Z","comments":true,"path":"2021/04/21/知识蒸馏/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/04/21/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/","excerpt":"","text":"知识蒸馏 知识蒸馏通常用于模型压缩，用一个已经训练好的模型A去“教”另外一个模型B。两个模型称为老师-学生模型。 通常模型A比模型B更强。在模型A的帮助下，模型B可以突破自我，学的更好。 Loss_Function_in_PyTorch softmax：输出是概率分布，(0,1) log_softmax：对sofmax取log，结果为负数 NLLLoss：输入：log_softmax，target，结果为正 CrossEntropy（交叉熵） 如上图所示，教师网络（左侧）的预测输出除以温度参数（Temperature）之后、再做softmax变换，可以获得软化的概率分布（软目标或软标签），数值介于0~1之间，取值分布较为缓和。 Temperature数值越大，分布越缓和；而Temperature数值减小，容易放大错误分类的概率，引入不必要的噪声。针对较困难的分类或检测任务，Temperature通常取1，确保教师网络中正确预测的贡献。 硬目标则是样本的真实标注，可以用one-hot矢量表示。 total loss设计为软目标与硬目标所对应的交叉熵的加权平均（表示为KD loss与CE loss），其中软目标交叉熵的加权系数越大，表明迁移诱导越依赖教师网络的贡献，这对训练初期阶段是很有必要的，有助于让学生网络更轻松的鉴别简单样本，但训练后期需要适当减小软目标的比重，让真实标注帮助鉴别困难样本。 另外，教师网络的推理性能通常要优于学生网络，而模型容量则无具体限制，且教师网络推理精度越高，越有利于学生网络的学习。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://yao-chen-ecnu.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"联邦学习","slug":"联邦学习","permalink":"https://yao-chen-ecnu.github.io/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"A Reliable and Accountable Privacy-Preserving Federated Learning Framework using the Blockchain","slug":"Poster  A Reliable and Accountable Privacy-Preserving Federated Learning Framework using the Blockchain","date":"2021-04-19T13:00:42.432Z","updated":"2021-04-22T07:09:09.145Z","comments":true,"path":"2021/04/19/Poster  A Reliable and Accountable Privacy-Preserving Federated Learning Framework using the Blockchain/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/04/19/Poster%20%20A%20Reliable%20and%20Accountable%20Privacy-Preserving%20Federated%20Learning%20Framework%20using%20the%20Blockchain/","excerpt":"","text":"摘要 联合学习有望支持涉及大数据集、大规模分布式数据所有者和不可靠网络连接的协作学习应用。为了保护数据隐私，现有的FL方法采用(k，n)-门限秘密共享方案，基于对客户端的半诚实假设，在本地模型更新交换中实现安全多方计算，以增加数据大小为代价处理随机客户端退出。这些方法对客户采用半诚实的假设，因此容易受到恶意客户的攻击。在这项工作中，我们提出了一个基于区块链隐私保护的联合学习框架，该框架利用了区块链的不变性和分散信任属性来提供模型更新的起源。我们基于BC的PPFL的概念验证实现证明了它对于联邦环境中本地模型更新的安全聚合是实用的。 引言 早期的FL设计假设中间结果，如随机梯度下降的参数更新，包含的信息比原始训练数据少[6]。因此，短暂的更新经常被暴露。然而，这些梯度可能会泄漏本地数据项的重要信息，尤其是当诸如数据结构的元数据可用时。为了解决这个问题，最近已经提出了几种用于FL的安全聚合算法[2，3]，利用秘密共享和差分隐私技术。然而，这些方法假设参与客户的松散联盟，这些客户可能加入或不加入学习任务，因此遭受随机客户退出。[3]提出采用(k，n)-门限秘密共享来提高对退出的鲁棒性。事实上，这些方案的安全保障植根于对客户的半诚实假设，即假设客户不得提交“假”更新，也不得相互勾结操纵学习过程或结果。。此外，几乎所有关于FL的现有工作都明确或隐含地采用简单的激励模型，该模型假设客户自愿参与协作学习，以他们的本地模型更新和计算资源换取改进的全局模型。这种扁平激励模型忽略了这样一个事实，即具有不同数据大小和计算能力的客户端在全局优化任务中做出不同的贡献，并且应该得到不同的奖励。 本文的主要贡献： 在这项工作中，我们提出了一个基于区块链隐私保护的联合学习框架。如图1所示，区块链互连了前端组件，如服务器、客户端和聚合器。它使用分布式交易分类账来记录关于前线任务、参与客户、本地和全球模型更新等的信息流。，在组件中。不可变分类帐通过跟踪每个FL任务中的数据流来支持数据起源，并提供了一个良好的信任基础来构建现有FL方法缺乏的验证机制。有了这样的验证机制，我们可以进一步将半诚实客户端假设扩展到更现实的恶意客户端假设，在这种假设下，一个客户端可以退出，提交虚假的本地更新，或者与其他恶意客户端串通。此外，利用分布式分类帐，服务器以及FL任务中的其他感兴趣的实体(例如，客户端和聚合器)可以跟踪每个客户端对全局优化的学习模型的贡献，这使得基于贡献的激励机制成为可能。在此基础上，我们可以进一步引入改进模型所有权的溢价，并相应地奖励矿工。 框架设计 1、联邦学习模型： 联邦平均模型 基于BC的PPFL的主要隐私目标是(a)保护本地数据私有；(b)保护本地数据不受服务器的影响，不将个别本地模型更新泄露给服务器；以及©保护本地和全局模型更新不受无关的内部(即聚合器、矿工)和外部实体的影响。同时，安全目标是确保(a)来自承诺客户的本地模型更新的机密性和完整性，以及(b)模型更新的来源。 2、架构设计 为了实现安全和隐私保护目标，我们提出了一个基于业务连续性的PPFL框架，它由五个组件组成，如图1所示。对于FL任务，服务器首先通告任务规范，例如应用程序的类型(例如，击键或活动预测)、设备的类型、训练数据的类型和格式(例如，陀螺仪或运动传感器数据、浮点格式)、学习模型的类型(例如，CNN)、计算要求(例如，学习速率)和任务设置(例如，聚合器、所需客户端的数量、批量大小等)。) 在任务事务中。愿意加入任务的客户端向聚合器注册，然后聚合器为所有提交的客户端创建一个提交事务。最后，矿工将所有交易记录到分类账中。 在FL任务期间，服务器首先与所有提交的客户端共享初始模型。为了模型隐私，这个初始模型在保护模式下共享。然后，客户端根据其本地训练数据计算本地模型更新，并将更新后的模型参数上传到聚合器。聚合器在同一批中收到的所有更新将被打包到本地更新事务中，并在矿工的帮助下记录到分类帐中。 3、出块 基于BC的PPFL依赖于三个构建块来提供FL中的数据隐私和出处 同态加密和代理重加密 现有的大多数隐私保护联合学习方法都采用秘密共享方案来实现安全多方计算，这种方法存在随机客户端退出的问题。为了解决这个问题，我们采用了由克莱姆和肖普[4]提出的具有两个陷门函数的Paillier密码系统的变体，以在模型交换期间保护局部模型 在我们的设计中，服务器为每个任务生成一对公共和私有任务密钥，并将公共任务密钥包含在任务规范中，而任务中涉及的每个聚合器生成一对公共和私有批处理密钥，并将公共批处理密钥分发到每个提交的客户端。任务密钥和批密钥以这样的形式构造，即批密钥是相应任务密钥的转换密钥，以支持代理重加密。因此，服务器不能直接恢复记录在分类帐中的单个本地更新，而是仅在聚合器聚合更新并用转换密钥重新加密后恢复聚合值。 区块链 值得注意的是，对于大型数据集上的大型模型更新，我们可以将加密的模型更新存储在分布式数据库中，如IPFS [1]，并且只记录区块链的地址。 验证 验证器可以批量检索客户端的所有更新，生成排除来自可疑客户端的输入的集合，并且比较验证前后全局模型的性能。在初步实验中，我们实现了基本验证功能，其中服务器充当验证器，在每轮恢复聚合后评估梯度。它使用损失函数比较更新后的全局模型与该轮初始模型的性能。 4、实现与评估 在Python中开发了Paillier密码系统的变体，以实现同态加密和代理重加密 区块链基于以太坊和truffle框架 使用PyTorch实现深度学习训练 我们使用简单的两层神经网络构建了二进制分类器，并使用来自UCI机器学习库的乳腺癌数据集作为训练数据。数据集有569个样本和30个特征。对于我们框架中使用的所有三种密码系统，我们生成了默认长度为2048位的公钥和私钥对。 我们在亚马逊EC2 t2.micro实例上运行了区块链，该实例具有1千兆内存和3.3千兆赫英特尔可扩展处理器。对于服务器、聚合器和客户端节点，我们采用了两种设置:在设置一中，我们在2 GHz英特尔酷睿i5处理器和3 GiB内存的笔记本电脑上运行所有三种类型的节点，它们可以从这里与亚马逊EC2上实现的区块链进行交互；在设置二中，我们也在亚马逊EC2实例上运行它们，这最小化了通信延迟，因为所有实例都在同一个云平台上。未来，我们将通过在多个云平台上部署不同的节点，进一步研究我们框架的可扩展性和通信延迟。 评估 我们首先在10个客户中随机划分训练数据，每个客户有50个来自IID划分数据的例子。然后，我们测量了不同FL操作的执行时间 我们重复了四次模型训练，并计算了单次迭代的平均执行时间，如表1所示。显然，在这两种设置下，在所有FL操作中，聚集梯度并将结果写入区块链是计算成本最高的操作。每次迭代的平均执行时间是所有FL操作的总和，在设置一和设置二中分别是1.62和1.27秒。这两种设置之间的差异表明了向区块链读写所引入的通信成本。 因此，我们将客户机批处理大小增加到20、30和40，并测量了执行时间。平均执行时间从10个客户端的1.62秒分别增加到20、30和40个客户端的1.73、2.36和2.43秒。这表明客户端数量的增加导致协议执行时间的次线性增加。 结论 在未来，我们将探索联合优化，在客户端的每个训练阶段结合小批量，并增加多客户端并行性，以实现目标测试集的准确性。我们还将考虑非IID分区数据，并根据客户的贡献调查奖励客户的激励计划。","categories":[{"name":"区块链+联邦学习论文阅读笔记","slug":"区块链-联邦学习论文阅读笔记","permalink":"https://yao-chen-ecnu.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"区块链+联邦学习","slug":"区块链-联邦学习","permalink":"https://yao-chen-ecnu.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"GFL A Decentralized Federated Learning Framework Based On Blockchain","slug":"GFL A Decentralized Federated Learning Framework Based On Blockchain","date":"2021-04-19T12:22:21.792Z","updated":"2021-04-22T10:04:29.945Z","comments":true,"path":"2021/04/19/GFL A Decentralized Federated Learning Framework Based On Blockchain/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/04/19/GFL%20A%20Decentralized%20Federated%20Learning%20Framework%20Based%20On%20Blockchain/","excerpt":"","text":"摘要 联合学习是一个快速发展的领域，已经提出了许多集中式和分散式的联合学习框架。然而，在恶意节点攻击下，如何提高通信性能、保持安全性和健壮性是当前FL框架面临的巨大挑战。在本文中，我们提出了Galaxy Federated Learning Framework(GFL)，一个基于区块链的分布式学习框架。**GFL引入了一致性哈希算法来提高通信性能，并提出了一种新的环分散算法(RDFL)来提高分布式的FL性能和带宽利用率。此外，GFL引入了星际文件系统(IPFS)和区块链，以进一步提高通信效率和FL安全性。**我们的实验表明，在恶意节点和非独立同分布(非IID)数据集的数据中毒情况下，GFL改进了通信性能和分布式FL性能。 引言 主要贡献： 本文设计了一种新的数据节点拓扑机制，采用了一致哈希算法。该机制能够显著降低通信压力，提高拓扑稳定性。 一种新的环形分散联邦学习(RDFL)算法，旨在提高带宽利用率以及分布式联邦学习的性能 为了提高分散式FL框架的通信性能和安全性，我们引入了IPFS技术来降低系统通信压力，引入了区块链技术来提高FL安全性。 框架介绍 1、环形分布式FL拓扑 一组nnn个数据节点，其中有mmm个为诚实可信任节点，其余n−mn-mn−m个为不可信任节点。这nnn个数据表示为DP1,DP2,DP3,...,DPn{DP_1,DP_2,DP_3,...,DP_n}DP1​,DP2​,DP3​,...,DPn​。GFL利用一致哈希算法来构建n个数据节点的环形拓扑。 一致性给哈希值Hk=Hash⁡(DPkip)⊆[0,232−1],DPkipH_{k}=\\operatorname{Hash}\\left(D P_{k}^{i p}\\right) \\subseteq\\left[0,2^{32}-1\\right], D P_{k}^{i p}Hk​=Hash(DPkip​)⊆[0,232−1],DPkip​ 代表了DPKDP_KDPK​的ip，k⊆[i,n]k\\subseteq[i,n]k⊆[i,n] 根据一致性哈希指，数据节点分布在环上，取值范围为[0,232−1][0,2^{32}-1][0,232−1] 不可信数据节点将把本地模型以顺时针方向发送到在环形拓扑上找到的最近的可信数据节点。图1显示了由一致哈希算法构建的环形拓扑。绿色数据节点代表可信数据节点，灰色数据节点代表不可信数据节点。根据顺时针原则，不可信数据节点DP2DP_2DP2​和DP3DP_3DP3​向可信数据提供者DP4DP_4DP4​发送模型。不可信数据节点DP5DP_5DP5​将模型发送到最近的可信数据节点DPkDP_kDPk​。 借助一致哈希算法，不同的不可信数据节点向不同的可信节点发送模型，有效降低了可信节点的通信压力。为了使环上可信节点的分布更加均匀，GFL引入了虚拟可信节点进一步降低通信压力，图2显示了带有虚拟节点的环形拓扑。 带有红色虚线的绿色节点代表虚拟节点。DP1v1DP^{v1}_1DP1v1​是DP1DP_1DP1​的虚拟节点。如果不可信数据节点发现最近的可信节点是顺时针方向的虚拟节点，则直接将模型发送给该虚拟节点对应的可信节点。 为什么要添加这个虚拟节点呢？ 2、RDFL算法 基于一致哈希算法构造的环分散拓扑，GFL开始执行RDFL算法。可信节点遵循图3所示的RDFL算法过程。MKM_KMK​表示数据节点DPkDP_kDPk​的模型。rrr表示执行模型同步的回合数，mmm表示可信节点数。 在每次迭代中 每个数据节点加载全局模型用于局部训练，并沿顺时针方向同步其模型 利用知识蒸馏将可信节点拥有的其他模型的暗知识收敛到局部模型，并在可信节点之间同步局部模型 每个可信节点执行联邦平均算法，生成新的全局模型并开始下一次迭代。 具体每一步的执行 （1）训练和同步模型 每个数据节点DPkDP_kDPk​首先加载上一次迭代生成的全局模型GMGMGM进行局部训练，得到一个新的局部模型MkM_kMk​，然后不可信节点根据顺时针原理将局部模型发送给最近的可信节点。此外，可信节点同步模型，使得所有可信节点获得剩余可信节点的模型。为了提高可信节点之间的带宽利用率，RDFL算法在一致性哈希算法中引入了Ring-allreduce和顺时针原则来实现模型同步。 从图1中可以看出，在同步开始之前，可信节点DP4DP_4DP4​接收来自不可信节点DP2DP_2DP2​和DP3DP_3DP3​的模型M2M_2M2​和M3M_3M3​。在第一轮同步之后（r=1r=1r=1），DP1DP_1DP1​ 获得了M1M_1M1​和MnM_nMn​，DP4DP_4DP4​获得了M1,M2,M3和M4M_1,M_2,M_3和M_4M1​,M2​,M3​和M4​,DPnDP_nDPn​获得了Mn和Mn−1M_n和M_{n-1}Mn​和Mn−1​，经过m−1m-1m−1轮同步后（r=m−1r=m-1r=m−1），所有可信节点已经获得了其他所有可信节点的模型 （2）知识蒸馏 由于同步，每个可信节点从剩余的可信数据节点获得模型。然后，每个可信节点在本地进行知识蒸馏。通过知识蒸馏，强大的教师模型中的暗知识可以转移到知识很少的学生模型中。学生模型的损失函数如公式所示 LCEL_{CE}LCE​表示交叉熵，DKLD_{KL}DKL​表示KL散度，PPP表示softmax之后的模型输出，zzz表示模型logit层的输出，TTT表示蒸馏温度的超参 知识蒸馏的目的就是最小化LstudentL_{student}Lstudent​，使得学生模型更好的学习暗知识 RDFL利用KL散度来衡量模型之间的分布差异。 在RDFL算法的知识蒸馏步骤中，每个可信节点DPkDP_kDPk​将本地模型MkM_kMk​作为学生，而DPkDP_kDPk​拥有的其余模型将被用作教师来学习暗知识。 为了防止遭到不可信节点的数据中毒，RDFL只允许与模型MkM_kMk​KL差异最小的前30%的模型作为教师模型 随着FL轮数的增加，教师模型的数量会动态增加，以提高泛化能力，但不超过50%。每个可信节点只保留局部模型，知识蒸馏后丢弃剩余模型。在图3中，受信任的节点DP1DP_1DP1​在蒸馏后得到本地模型M1M_1M1​，DP4DP_4DP4​得到M4M_4M4​，DPnDP_nDPn​得到MnM_nMn​。然后，可信节点利用Ring-allreduce和顺时针原则来同步可信节点的本地模型。同步后，所有可信节点都有其他可信节点的本地模型。 （3）联邦平均 在这一步中，每个可信节点运行 FedAvg获得一个新的全局模型。然后，每个可信节点根据环形拓扑将新的全局模型逆时针发送给不可信节点，并开始下一次迭代。 3、IPFS和区块链 在传统的分布式FL算法中，数据节点间模型的传递占用大量通信开销，造成严重的通信压力。此外，这些通信正遭受篡改的安全风险，并且无法追踪模型的来源。 （1）通信压力 为了减轻沟通压力，GFL引入IPFS作为模型存储系统。IPFS由GFL的可信节点组成。IPFS的文件将被分成存储在不同节点上的多个部分，IPFS将生成对应于该文件的IPFS哈希。IPFS哈希是一个46字节的字符串，相应的文件可以通过IPFS哈希从IPFS获得。 （2）通信安全 为了提高分散的FL框架的安全性，GFL引入了区块链作为通信系统。 在GFL，数据节点发送的IPFS哈希以区块链事务的形式传输。每笔交易都通过一致算法进行验证，以降低篡改风险。此外，由于区块链记录了每个交易信息，GFL可以根据交易信息追溯到恶意节点。 值得注意的是，由于区块链的开放性，区块链的任何节点都可以获得区块链的交易信息。为了避免IPFS散列的泄露，GFL对IPFS散列进行加密，以保护数据节点的隐私。 如何进行加密，密钥如何保存 框架设计 本节详细描述了GFL的体系结构和加密机制。 （1）架构 GFL由IPFS和区块链组成。就区块链而言，GFL利用以太坊作为区块链的实施。在GFL，区块链由所有数据节点组成，IPFS由可信节点组成。 智能合约是部署在区块链的协议。在GFL，数据节点通过调用智能合约的函数，将模型的IPFS哈希以事务的形式发送给区块链。区块链首先执行共识机制来验证事务是否被篡改，如果验证通过，事务将被打包成块。 （2）加密机制 由于区块链的可见性，区块链的节点可以访问任何块中的交易信息，不可信数据节点中恶意节点的存在可能导致数据隐私泄露。为解决数据隐私泄露问题，GFL利用RSA非对称加密算法和AES对称加密算法对交易信息进行加密。 根据RDFL算法，数据节点DPh,h⊆[1,n]DP_h,h\\subseteq[1,n]DPh​,h⊆[1,n]将模型的IPFS哈希发送给它最近的可信节点DPk,k⊆(h,n]DP_k,k\\subseteq(h,n]DPk​,k⊆(h,n]，如果DPhDP_hDPh​和DPkDP_kDPk​是第一次通信，DPkDP_kDPk​生成一个AES密钥，并且使用DPhDP_hDPh​的公钥加密它，然后发送给DPhDP_hDPh​。DPhDP_hDPh​使用私钥解密得到这个AES密钥。然后IPFS哈希在DPhDP_hDPh​和DPkDP_kDPk​之间传输时就用AES密钥来进行加密。 这里不多此一举嘛？为什么不直接用非对称加密呢？DPhDP_hDPh​向DPkDP_kDPk​发送时就用DPkDP_kDPk​的公钥，反过来就用DPhDP_hDPh​的公钥加密，各自收到消息之后用自己的私钥解密就可以了啊。 实验和结果 在这一部分中，我们首先评估RDFL算法的分散FL性能，然后评估GFL的通信耗时和通信数据量。 1、RDFL 算法分析 我们通过仿真实验验证了RDFL算法的性能。模拟实验的设置如下: 数据集：CIFAR-10、CIFAR-100和MNIST 在这个实验中，我们模拟了5个(n = 5)数据节点。验证RDFL在恶意节点数据中毒下的FL性能。MNIST数据集被模拟为恶意数据集。我们将数据集划分为独立的同分布(IID)数据集，并将它们分配给数据节点。此外，为了验证算法在非独立同分布数据集的FL性能，我们利用潜在狄利克雷分配(LDA)和标签划分方法将数据集划分为非独立同分布数据集。当使用标签分区方法时，每个分区仅包括两类CIFAR-10或二十类CIFAR-100数据集。 模型设置：我们使用一个卷积神经网络(CNN)，它有三个3×3卷积层(第一个有32个通道，第二个有64个通道，第三个有64个通道，每个通道都有2×2最大池和ReLu激活)和两个FC层。 训练设置：在每个数据节点，我们使用SGD算法来训练上面提到的CNN模型。学习率和权重衰减都是0.001。批量为64。在每一轮FL中，数据节点执行5轮本地模型训练。 比较设置：我们假设所有不可信节点都是恶意节点，按照4种不同的比例在5个数据节点中设置恶意数据节点，比较RDFL算法和FedAvg的性能。此外，我们还比较了RDFL算法和FedAvg在无恶意节点的不同非IID数据集下的性能。 结果： 为了验证RDFL算法在恶意节点数据中毒情况下的分布式FL性能，我们进行了四种节点比率下的RDFL算法和FedAvg。图5显示了训练的过程，表1显示了在不同的节点比率下，RDFL算法比FedAvg具有更好的性能。 此外，为了验证RDFL算法在非IID数据集下的性能，我们使用潜在狄利克雷分配(LDA)和标签划分方法将数据集划分为5个分区，以比较RDFL算法和FedAvg的性能。图5显示了培训的过程。从表1可以看出，RDFL算法在非IID数据集上具有更好的性能。 2、通信分析 通过仿真实验验证了GFL的通信性能。模拟实验的设置如下: 实验设置：我们在IPFS和区块链建立了一个具有3个可信数据节点的网络(内存:16GBCPU:Intel Xeon Gold 5118 2.30GHz)，并利用Lenet-5型号和ResNet-50型号验证GFL的通信性能。 实验结果： 我们首先验证了传统分散FL框架的通信性能，然后验证了仅引入时的通信性能。最后验证了IPFS和区块链同时引入IPFS时GFL的通信性能。表2和表3显示，IPFS大大减少了通信时间和通信数据量。此外，由于区块链的共识算法，GFL引入区块链后，通信时间消耗略有增加。总之，GFL比传统的分布式FL框架有更好的通信性能。 结论 在本文中，我们提出了一个基于区块链的分布式FL框架，称为GFL，以解决现有的分布式FL框架的问题。GFL利用一致哈希算法和RDFL算法来提高通信性能、分布式的FL性能和稳定性。此外，GFL引入了IPFS和区块链，以进一步提高通信性能和FL安全性。","categories":[{"name":"区块链+联邦学习论文阅读笔记","slug":"区块链-联邦学习论文阅读笔记","permalink":"https://yao-chen-ecnu.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"区块链+联邦学习","slug":"区块链-联邦学习","permalink":"https://yao-chen-ecnu.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"A Blockchain-based Decentralized Federated Learning Framework with Committee Consensus","slug":"A Blockchain-based Decentralized Federated Learning Framework with Committee Consensus","date":"2021-04-19T06:57:34.057Z","updated":"2021-04-19T12:22:38.644Z","comments":true,"path":"2021/04/19/A Blockchain-based Decentralized Federated Learning Framework with Committee Consensus/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/04/19/A%20Blockchain-based%20Decentralized%20Federated%20Learning%20Framework%20with%20Committee%20Consensus/","excerpt":"","text":"摘要 联邦学习已经被广泛研究并应用于各种场景。在移动计算场景中，联合学习保护用户不暴露他们的私有数据，同时协作地为各种现实世界的应用程序训练全局模型。然而，由于恶意客户端或中央服务器不断攻击全局模型或用户隐私数据，联邦学习的安全性日益受到质疑。为了解决这些安全问题，我们提出了一个基于区块链的分布式联邦学习框架，即一个基于区块链的具有委员会共识的联邦学习框架。该框架将区块链用于全局模型存储和本地模型更新交换。为了实现提出的BFLC，我们还设计了一种创新的委员会共识机制，可以有效减少共识计算量，减少恶意攻击。然后我们讨论了BFLC的可扩展性，包括理论安全性、存储优化和激励。最后，我们使用真实数据集进行了实验，以验证BFLC框架的有效性。 引言 传统联邦学习的缺点： 在FL设置中，服务器执行更新聚合、客户端选择、全局模型维护等中心操作。服务器需要从众多客户端收集更新来完成聚合操作，还需要向这些客户端广播一个新的全局模型，这就对网络带宽提出了很高的要求。此外，基于云的服务器受到云服务提供商稳定性的影响。集中式服务器可以通过偏向某些客户端来扭曲全局模型。此外，一些恶意的中央服务器可以毒害模型，甚至从更新中收集客户端的隐私数据。因此，中央服务器的稳定性、公平性和安全性对FL至关重要。 本文的主要贡献： 提出了一个基于区块链的FL框架BFLC，它详细定义了模型存储模式、训练过程和新的委员会共识。 从技术上讨论了BFLC的可扩展性，包括社区中的节点管理、恶意节点攻击的分析和存储优化。 通过在真实的FL数据集上的实验证明了BFLC的有效性。我们还通过模拟恶意攻击验证了BFLC的安全性。 相关工作 目前区块链应用于联邦学习的三个挑战： 共识效率。对于基于区块链的方法来说，为每个块达成共识是一个不可避免的过程。考虑到FL设置中的大量学习节点，广播共识非常耗时。因此，降低共识成本不是小事。相关作品中有一篇[10]选择了一位领导者来执行共识。然而，该标准依赖于许多外部数据。 模型安全性。该框架应防止全球模型暴露于未经授权的设备和中毒。系统的安全性很少在基于区块链的FL环境下进行研究。 框架可扩展性。当将这些训练框架应用到现实世界的应用中时，我们总是需要添加细节规则来适应不同的场景。因此，框架的可扩展性决定了它们的应用范围。 本文提出的框架 A. 区块链存储 为了实现权限控制，BFLC的存储是一个联盟区块链系统，只有授权的设备才能访问FL训练内容。在区块链，我们设计了两个不同的块来存储全局模型和局部更新(如图2所示)，它们统称为学习信息。为了简单起见，我们假设一个块中只放置一个学习信息。 一开始，一个随机初始化的模型被放入#0块，然后第0轮训练开始。节点访问当前模型并执行局部训练，并将验证后的局部梯度放入新的更新块。当连续有足够的更新块时，智能契约触发聚合，下一轮的新模型被生成并放置在链上。我们要注意的是，FL训练只依赖最新的模型块，存储历史块是为了故障回退和块验证。 假设每轮需要kkk个更新，共进行ttt轮迭代，那么#t∗（k+1）t*（k+1）t∗（k+1）块代表第t轮的模型块，#[t∗(k+1)+1,(t+1)∗(k+1)−1][t*(k+1)+1,(t+1)*(k+1)-1][t∗(k+1)+1,(t+1)∗(k+1)−1]为第ttt轮迭代的更新块 模型块：块头，迭代轮数ttt，和全局模型 更新块：块头，迭代轮数ttt，本地更新模型，上传地址和更新分数 B. 委员会共识机制 选举出几个诚实的节点组成一个验证委员会负责模型验证和共识出块，其他节点进行本地训练并将模型上传给共识委员会。然后，委员会验证更新，并给它们打分。只有合格的更新将被打包到区块链。在下一轮开始时，根据上一轮的节点分数选举一个新的委员会。 委员会如何进行验证： 委员会成员通过将他们的数据视为一个验证集来验证本地更新，验证的准确性成为分数。这是最小化的方法，不需要委员会的进一步操作，只需要运行学习模型的基本能力。综合各委员会成员的得分后，中位数将成为本次更新的得分。 BFLC可以实现这些优点: 高效率:只有几个节点会验证更新，而不是广播到每个节点并达成协议。 K倍交叉验证:委员不会全面参加当地培训。因此，委员会的本地数据被视为验证集。随着每轮委员会成员的交替，验证集也随之改变。在这种情况下，在FL上实现了k倍交叉验证。 反恶意:基于验证分数，智能合约将选出相应的表现较好的节点，组成下一轮培训的新委员会。这意味着所选择的本地数据分布是群居的，并且该节点不是恶意的。 委员会共识机制描述不够具体。最初的委员会怎么选举，如何判断委员会成员是诚实的，万一委员会节点中混入恶意节点，委员会节点本地运行模型会不会造成节点的模型泄露，根据怎样的具体标准进行打分，如何保证委员会的打分是可信的？ C. 模型训练 在FL中，为了安全和隐私，原始数据将保存在本地的节点中，这些节点只将梯度上传到区块链。 存在两点挑战： 本地数据分布可能不是独立且同分布的(非IID)—&gt;每轮只需要一定数量的局部更新 设备不总是可用的（可能存在掉线的情况）-----&gt;节点随时主动获取当前最新的全局模型进行训练，当节点贡献局部梯度更新时，由委员会验证，验证通过则为其发放奖励 每一轮，当委员会验证足够的本地更新时，聚合过程被激活。委员会将这些经过验证的更新汇总成一个新的全局模型。新的全局模型写入区块链后，委员会将再次选举，下一轮训练开始。 讨论 A. 节点管理和奖励机制 节点管理 为了控制权限，指定组成训练社区的初始节点负责节点管理，即作为管理者。在加入训练社区之前，每个设备都必须经过经理的验证。此验证处于黑名单模式:如果设备因不当行为(如提交误导性更新、传播私人模型)被踢出社区，设备将被拒绝。 根据建议的区块链存储结构，在新节点加入后，可以在链上快速找到最新的全局模型。节点可以立即使用模型完成本地任务，也可以用本地数据更新模型，经共识委员会验证后在链上获得分数。 奖励机制 社区中的节点总是可以在不提交更新的情况下使用该模型，因此需要有效的激励来鼓励节点向全局模型提供更新。为了解决这个问题，我们提出了一种叫做按贡献分享利润的激励机制。 许可费:每台设备都要为全局模型的访问许可付费，这些费用由管理者保管。然后，节点可以无限制地访问社区中的最新模型。 利润分享:每轮汇总后，经理根据提交更新的分数将奖励分配给相应的节点。 因此，频繁提供更新可以获得更多的回报，不断更新的全局模型将吸引更多的节点参与。这种激励机制具有很高的可扩展性，以适应不同的现实世界应用。 B. 委员会选举 在每一轮结束时，从经验证的更新提供者中选出一个新的委员会。 随机选举:从经过验证的节点中随机选择新的委员会成员。从机器学习的角度来看，这种方法提高了模型的泛化能力，减少了过拟合。然而，当恶意节点伪装成正常节点时，对恶意攻击的抵抗力较弱。 分数选举:认证分数最高的供应商组成新的委员会。由于委员会中缺少部分节点，这可能会加剧样本的不均匀分布。然而，对于恶意节点攻击，这种方法显著增加了攻击成本，并带来了更多的安全性和稳定性。 多因素优化:这种方法考虑了设备的多种因素(即网络传输速率)和最佳选择的验证分数。但是，这种优化会带来额外的计算开销。因此，应该根据实际情况和相关要求来应用这种方法。 C. 恶意节点 恶意节点被定义为提交不正确的恶意模型更新的节点。 我们将所有节点的数量表示为NNN，其中委员会成员的数量为MMM，其余的N−MN-MN−M个节点为训练节点。当且仅当超过m/2m/2m/2个委员会成员合谋时，而已更新才会被聚合。然而在上一轮中，委员会成员时表现最好的M个成员，这就意味着这些恶意委员会成员的更新倍上一轮委员会中超过m/2m/2m/2个节点接受，这是一个无线循环过程，因此只要第一轮的委员会中有超过m/2m/2m/2诚实节点，就没有恶意节点可以进入委员会并且损害全局模型。 考虑另外一个极端情况：恶意节点通过伪装成正常节点合谋赢得委员会席位。当恶意节点占据一半席位时，攻击开始。 分析:参与节点数量为AAA，AAA中恶意节点占的百分比为q,q∈(0,1)q,q\\in(0,1)q,q∈(0,1),委员会成员占的百分比为p∈(0,1)p\\in(0,1)p∈(0,1)，攻击目标是抢占委员会中超过A∗P/2A*P/2A∗P/2的席位。假设每个节点的性能都是相似的，攻击成功事件就可以描述为：委员会节点A∗pA*pA∗p个中，有超过半数来自A∗pA*pA∗p，设定A为1000 我们应该注意到，只有当恶意百分比大于50%时，攻击成功的概率才能明显大于0。这个结论类似于区块链电力系统51%的攻击。换句话说，在一个分布式社区中，恶意节点应该拥有51%的计算资源来攻击系统，而代价远远超过好处。此外，历史模型和更新存储在区块链上，因此，在攻击发生后，可以选择回撤。 D. 存储优化 在实际应用中，存储开销是决定训练设备硬件要求的一个重要因素。基于上述区块链存储方案(如图2所示)，可以快速找到最新的全局模型。虽然历史模型和更新可以提供恢复功能，但也占用了巨大的存储空间。 这里我们给出一个简单可行的存储开销降低方案:容量不足的节点可以在本地删除历史块，只保留最新的模型和本轮的更新。这样可以解决一些节点存储空间不足的问题，同时在核心节点上保留了灾难恢复和块验证的能力。但是这种方法的缺点也很明显。区块链的可信度随着节点的删除而降低。在相互不信任的培训社区中，出于安全考虑，每个节点都不能使用该方案。 因此，可信可靠的第三方存储可能是更好的解决方案。区块链仅维护每个模型或更新文件所在的网络地址和修改操作记录。其他节点与集中式存储交互，以获取最新型号或上传更新。该集中式存储将负责灾难恢复备份和分布式文件存储服务。 实验 数据集：FEMNIST 该数据集包含用于手写字符图像分类任务的80，5263个样本和3550个用户，并且包含62个不同的类别(10个数字，26个小写，26个大写)。 模拟了900个设备，其中本地数据集在数量上是不平衡的，并且在分布上不是独立的 区块链系统选用FISCO，是一个基于PBFT共识的一个去跨链平台 智能合约使用solidity语言进行开发 学习模型用Python 3.7.6和Tensorflow 1.14.0编写，在Geforce RTX 2080 ti GPU上执行 我们将BFLC与基础FL [12]框架和独立培训框架作为基线进行比较。每个框架执行经典的图像分类模型AlexNet [14]作为全局模型，并固定一组模型超参数以确保公平性。在实验设置方面，我们将每轮活动节点的比例定义为k%，其中40%将在下一轮BFLC中当选为委员会成员。基础FL的训练节点比例也是k%。同时，独立训练将利用整个数据集。在不同k值的条件下，我们将他们的表现记录在表一中。 从表一可以看出，随着活动节点比例的增加，BFLC的性能不断接近基本FL框架的效果，与数据集完整的独立训练相比，只有轻微的损失。值得一提的是，BFLC可以通过委员会共识机制显著降低共识的消耗。 恶意攻击下的实验 我们假设恶意节点的攻击模式是带有点态高斯随机噪声的随机扰动。 基本FL不会执行任何防御措施，随机选择的主动节点生成的模型更新会被聚合。CwMed构建一个全局梯度，其中每个条目是具有相同坐标的局部梯度中条目的中间值。BFLC依靠上面提到的委员会共识来抵抗攻击。每次更新都会从委员会获得一个分数(即局部预测精度的中位数)。 为了增强攻击的有效性，我们假设恶意节点是共谋的，即恶意委员会的成员会对恶意更新给予随机的高分(例如90%̘100%)。活跃节点比例定为10%，下一轮选举20%为委员会。如图4所示，BFLC可以抵抗比比较方法高得多的恶意节点比例。这表明，在委员会机制的帮助下，边境事务局的工作卓有成效。 结论 基于一个可信的区块链系统，我们提出了BFLC，这是一个利用委员会共识的分散的、联合的学习框架。这样的委员会共识可以有效避免恶意中心服务器或恶意节点的影响。在实验部分，我们采用真实数据集验证了BFLC框架的有效性，该数据集可以获得类似于联邦学习中集中训练的全局模型。我们还讨论了BFLC的可扩展性，它在安全性、数据存储和激励机制方面具有广阔的研究前景。","categories":[{"name":"区块链+联邦学习论文阅读笔记","slug":"区块链-联邦学习论文阅读笔记","permalink":"https://yao-chen-ecnu.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"区块链+联邦学习","slug":"区块链-联邦学习","permalink":"https://yao-chen-ecnu.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"Biscotti A Blockchain System for Private and Secure Federated Learning","slug":"Biscotti A Blockchain System for Private and Secure Federated Learning","date":"2021-04-15T04:35:08.489Z","updated":"2021-04-19T08:35:05.973Z","comments":true,"path":"2021/04/15/Biscotti A Blockchain System for Private and Secure Federated Learning/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/04/15/Biscotti%20A%20Blockchain%20System%20for%20Private%20and%20Secure%20Federated%20Learning/","excerpt":"","text":"摘要 联合学习是支持安全多方机器学习(ML)的最新技术:数据保存在所有者的设备上，模型的更新通过安全协议进行聚合。然而，这个过程假设一个可信的集中式基础设施来进行协调，并且客户端必须相信中央服务不使用客户端数据的副产品。除此之外，一组恶意客户端还可能通过执行中毒攻击来损害模型的性能。作为回应，我们提出了Biscotti:一种完全分散的对等(P2P)多方消息传递方法，它使用区块链和加密原语来协调对等客户端之间的隐私保护消息传递过程。我们的评估表明，Biscotti是可扩展的、容错的，并且能够抵御已知的攻击。例如，当系统中存在30%的对手时，Biscotti能够保护单个客户端更新的隐私，并保持全局模型的大规模性能。 背景 机器学习应用中的一个常见需求是收集大量的训练数据。这些数据经常是分布式的，例如在医院或物联网部署中的设备之间。然而，当在多方环境中训练ML模型时，用户必须与集中式服务共享他们潜在的敏感信息。这种共享对于不愿意信任第三方的用户或公司来说是有问题的。例如，制药公司在药物发现方面相互竞争，很少共享数据。此外，互联网用户越来越意识到他们的数据的价值，并希望保持对其数据的控制。为了避免直接共享敏感数据，联合学习是大规模安全多方ML的一个突出解决方案:客户端通过可信的聚合器来训练共享模型，而不会暴露其底层数据或计算。 攻击： 毒化攻击 窃取隐私：对手也可以在联合学习中攻击其他客户端的隐私:在信息泄漏攻击中，对手伪装成诚实的数据提供者，并试图通过观察目标的共享模型更新来推断目标客户端的敏感训练数据的属性 先前的解决办法：集中式的异常检测、差分隐私、安全聚合 但据我们所知，同时解决这两种威胁的私有分散解决方案尚不存在。 此外，这些方法不适用于缺乏可信的中央权威的分散环境。因为ML不需要强一致或一致性来收敛，所以传统的强一致协议，如拜占庭容错(BFT)协议对机器学习工作负载的限制过于严格。 主要的贡献： 我们的主要贡献是将几种现有技术整合到一个连贯的系统中，在高度分布式P2P环境中提供安全和私有的多方机器学习。特别是，比肖蒂通过多克鲁姆防御防止对等体毒害模型[42]，通过不同的私人噪音提供隐私[22]，[23]，并使用沙米尔秘密进行安全聚合[43]。 我们发现Biscotti可以在266.7分钟内在60000个图像数据集上训练一个有200个同伴的MNIST softmax模型，同时可以抵抗高达30%的敌对同伴。此外，我们还证明了Biscotti的设计对于需要了解客户端SGD更新的信息泄漏攻击[13]是有弹性的，而且Biscotti对于之前工作中的中毒攻击[44]也是有弹性的。Biscotti还具有容错能力，可以应对每1.875秒就会发生故障的节点，并提供模型训练，即使节点发生故障也可以收敛。 挑战与关键技术 女巫攻击：基于VRF和POF的一致性哈希 毒化攻击：使用Multi-Krum进行验证更新 信息窃取：差分隐私 隐私性：安全聚合和密钥共享 一些假设 设计假设 POF是基于POS的 区块链拓扑 机器学习：使用SGD 攻击假设 在女巫攻击中，我们假设对手控制多个参与方，但不超过总体的30% 将中毒攻击限制为错误标记数据，导致训练过的模型对其进行错误分类，不包括后门攻击和基于梯度上升的攻击 当对手进行信息泄漏攻击时，我们假设他们的目标是学习受害者的本地数据集的属性。具体地说，我们提供了记录级别的隐私，这可以防止从用户数据集中对单个示例进行反匿名化。由于安全聚合的漏洞，我们不考虑带有边信息的信息泄漏攻击，也不考虑试图学习整个目标类属性的类级隐私攻击。 BISCOTTI设计 设计目标： 收敛到最优全局模型(在联邦学习设置中没有对手的情况下训练的同一模型); 通过验证对等体模型更新来防止中毒; 通过防止信息泄露攻击来保持对等体训练数据的私密性 总体设计： 每出一个块代表完成一次SGD迭代 1：每个peer在本地计算SGD更新值，即梯度 2：每个peer从一个noising peers中获取噪声，这个noising peers是由VRF算法挑选出来的 3：每个peer为自己的梯度添加上噪声，那么现在得到了masked update 4：然后这个masked update会被验证委员会(verification committee)进行验证，该验证委员会也是由VRF算法选举出来的，如果peer的masked update可以通过Multi-KRUM算法的验证，那么验证委员会的每一个成员会为该peer的unmasked update签署一个承诺 5：如果大多数委员会的成员都签署了一个更新 6：那么这个更新就会通过Shamir secret shares协议分成多份 7：然后多份更新就会被发送到一个聚合委员会(aggregation committee)中进行聚合，聚会委员会会执行一个安全的协议去聚合unmaked update。 完成聚合后所有对梯度有贡献的peers以及担任了验证和聚合委员会的都会受到额外的奖励。 8：聚合后的updates添加到全局的模型中，并存储在一个新的区块中，同时将更新后的模型会广播给所有的peers，并且新产生的区块会被添加到账本上 训练初始化 创世块由可信任的第三方创立 包含以下信息，所有peers都可以获取到创世块中的信息： 初始模型状态w0和预计T的迭代次数 用于创建SGD更新承诺的公钥PK(见附录C) 系统中所有其他对等体的公钥,用于提交和验证签名验证期间 每个peer的噪声(见图3和附录B) 初始的权益分配 当有新的区块被追加时执行的权益更新功能。 区块链设计 为了验证聚合是真实计算的，需要在块中包含单个更新。然而，单独存储它们会泄露个人私人训练数据[13]、[20]的信息。使用polynomial commitments 多项式承诺进行SGD更新，并将其映射到椭圆曲线上的一个点(详见附录C)。承诺通过隐藏个体更新来提供隐私，但是可以同态组合以验证aggregatorP㼿wiwas对全局模型的更新是诚实计算的。如果提交的更新列表等于总和，则以下等式成立: 根据权益进行角色选举","categories":[{"name":"区块链+联邦学习论文阅读笔记","slug":"区块链-联邦学习论文阅读笔记","permalink":"https://yao-chen-ecnu.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"区块链+联邦学习","slug":"区块链-联邦学习","permalink":"https://yao-chen-ecnu.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"共识算法","slug":"共识算法","date":"2021-03-24T08:40:28.069Z","updated":"2021-06-22T02:48:19.997Z","comments":true,"path":"2021/03/24/共识算法/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/03/24/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/","excerpt":"","text":"区块链共识算法之Raft&amp;Pbft&amp;Rbft 什么是共识算法 区块链技术中，共识算法是其中核心的一个组成部分。 **什么是共识？**从两个层面理解共识：第一个层面是点，即多个节点对某个数据达成一致共识。第二个层面是线，即多个节点对多个数据的顺序达成共识。其中对数据顺序达成一致共识是很多共识算法要解决的根本问题。 共识算法根据区块链的分类同样可以分成三大类：公链，联盟链和私链。 私链：私链的适用环境一般是不考虑集群中存在作恶节点，只考虑因为系统或者网络原因导致的故障节点。如paxo，raft。 联盟链：联盟链的适用环境除了需要考虑集群中存在故障节点，还需要考虑集群中存在作恶节点。对于联盟链，每个新加入的节点都是需要验证和审核的。如pbft。 公链：公链不断需要考虑网络中存在故障节点，还需要考虑作恶节点。公链中的节点可以很自由的加入或者退出，不需要严格的验证和审核。 一、raft和pbft的最大容错节点数 故障节点：节点因为系统繁忙、宕机或者网络问题等其他异常情况导致的无响应。 作恶节点：除了可以对集群的其他节点的请求无响应之外，还可以故意的发送错误的数据，或者给不同的其他节点发送不同的数据，使整个集群的节点最终无法达成共识。 raft是针对私链的共识算法，所以raft的容错只支持容错故障节点，不支持容错作恶节点。假设集群总节点数为nnn，故障节点为 fff ，根据小数服从多数的原则，集群里正常节点只需要比 fff 个节点再多一个节点，即 f+1f+1f+1个节点，正确节点的数量就会比故障节点数量多，那么集群就能达成共识。因此 raft 算法支持的最大容错节点数量是(n−1)/2(n-1)/2(n−1)/2。 对于 pbft 算法，因为 pbft 算法的除了需要支持容错故障节点之外，还需要支持容错作恶节点。假设集群节点数为 NNN，有问题的节点为 fff。有问题的节点中，可以既是故障节点，也可以是作恶节点，或者只是故障节点或者只是作恶节点。那么会产生以下两种极端情况： 第一种情况，fff 个有问题节点既是故障节点，又是作恶节点，那么根据小数服从多数的原则，集群里正常节点只需要比fff个节点再多一个节点，即 f+1f+1f+1个节点，确节点的数量就会比故障节点数量多，那么集群就能达成共识。也就是说这种情况支持的最大容错节点数量是 (n−1)/2(n-1)/2(n−1)/2。 第二种情况，故障节点和作恶节点都是不同的节点。那么就会有 fff个故障节点和fff个作恶节点，当发现节点是故障节点后，会被集群排除在外，剩下fff个作恶节点，那么根据小数服从多数的原则，集群里正常节点只需要比fff个节点再多一个节点，即 f+1f+1f+1个节点，正确节点的数量就会比作恶节点数量多，那么集群就能达成共识。所以，所有类型的节点数量加起来就是f+1f+1f+1 个正确节点，fff个故障节点和fff个问题节点，即 3f+1=n3f+1=n3f+1=n。 结合上述两种情况，因此 pbft 算法支持的最大容错节点数量是(n−1)/3(n-1)/3(n−1)/3。 二、raft算法 raft算法包含三种角色，分别是：跟随者（follower）、候选人（candidate）和领导者（leader）。 leader: 处理所有客户端交互，日志复制等，一般一次只有一个Leader. follower: 类似选民，完全被动 candidate候选人: 类似Proposer律师，可以被选为一个新的领导者。 集群中的任意节点在某一时刻只能是这三种状态的其中一种，这三种角色是可以随着时间和条件的变化而相互转换的。 每个节点上都有一个倒计时器 (Election Timeout)，时间随机在 150ms 到 300ms 之间。有几种情况会重设 Timeout： 收到选举的请求 收到 Leader 的 Heartbeat raft算法主要又两个过程：第一个是领导者选举，第二个是日志复制，其中日志复制过程分为记录日志和提交数据两个阶段。 算法流程参考：https://www.jianshu.com/p/8e4bbe7e276c Raft 官网：https://raft.github.io/ Raft 原理动画 (推荐看看)：http://thesecretlivesofdata.com/raft/ Raft 算法解析图片来源：http://www.infoq.com/cn/articles/coreos-analyse-etcd 三、pbft 算法 的提出主要是为了解决拜占庭将军问题。","categories":[{"name":"区块链","slug":"区块链","permalink":"https://yao-chen-ecnu.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://yao-chen-ecnu.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"从僵尸游戏学Solidity","slug":"从僵尸游戏学Solidity（笔记）","date":"2021-03-15T12:45:57.437Z","updated":"2021-03-22T05:36:01.500Z","comments":true,"path":"2021/03/15/从僵尸游戏学Solidity（笔记）/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/03/15/%E4%BB%8E%E5%83%B5%E5%B0%B8%E6%B8%B8%E6%88%8F%E5%AD%A6Solidity%EF%BC%88%E7%AC%94%E8%AE%B0%EF%BC%89/","excerpt":"","text":"写在前面 学习地址：https://cryptozombies.io/zh/course 智能协议的永固性 （即以太坊上的 DApp 跟普通的应用程序的区别） 在你把智能协议传上以太坊之后，它就变得不可更改, 这种永固性意味着你的代码永远不能被调整或更新。你编译的程序会一直，永久的，不可更改的，存在以太坊上。这就是 Solidity 代码的安全性如此重要的一个原因。如果你的智能协议有任何漏洞，即使你发现了也无法补救。你只能让你的用户们放弃这个智能协议，然后转移到一个新的修复后的合约上。 但这恰好也是智能合约的一大优势。代码说明一切。如果你去读智能合约的代码，并验证它，你会发现，一旦函数被定义下来，每一次的运行，程序都会严格遵照函数中原有的代码逻辑一丝不苟地执行，完全不用担心函数被人篡改而得到意外的结果。 Gas - 驱动以太坊DApps的能源 在 Solidity 中，你的用户想要每次执行你的 DApp 都需要支付一定的 gas，gas 可以用以太币购买，因此，用户每次跑 DApp 都得花费以太币。 一个 DApp 收取多少 gas 取决于功能逻辑的复杂程度。每个操作背后，都在计算完成这个操作所需要的计算资源，（比如，存储数据就比做个加法运算贵得多）， 一次操作所需要花费的 gas 等于这个操作背后的所有运算花销的总和。 省 gas 的招数：结构封装 （Struct packing） 除了基本版的 uint 外，还有其他变种 uint：uint8，uint16，uint32等。 通常情况下我们不会考虑使用 uint 变种，因为无论如何定义 uint的大小，Solidity 为它保留256位的存储空间。例如，使用 uint8 而不是uint（uint256）不会为你节省任何 gas。 除非，把 uint 绑定到 struct 里面。 如果一个 struct 中有多个 uint，则尽可能使用较小的 uint, Solidity 会将这些 uint 打包在一起，从而占用较少的存储空间。 12345678910111213struct NormalStruct &#123; uint a; uint b; uint c;&#125;struct MiniMe &#123; uint32 a; uint32 b; uint c;&#125;&#x2F;&#x2F; 因为使用了结构打包，&#96;mini&#96; 比 &#96;normal&#96; 占用的空间更少NormalStruct normal &#x3D; NormalStruct(10, 20, 30);MiniMe mini &#x3D; MiniMe(10, 20, 30); “view” 函数不花 “gas” 这是因为 view 函数不会真正改变区块链上的任何数据 - 它们只是读取。因此用 view 标记一个函数，意味着告诉 web3.js，运行这个函数只需要查询你的本地以太坊节点，而不需要在区块链上创建一个事务（事务需要运行在每个节点上，因此花费 gas）。 注意：如果一个 view 函数在另一个函数的内部被调用，而调用函数与 view 函数的不属于同一个合约，也会产生调用成本。这是因为如果主调函数在以太坊创建了一个事务，它仍然需要逐个节点去验证。所以标记为 view 的函数只有在外部调用时才是免费的。 使用 SafeMath预防溢出 为了防止这些情况，OpenZeppelin 建立了一个叫做 SafeMath 的 库(library)，默认情况下可以防止这些问题。 一个_库_ 是 Solidity 中一种特殊的合约。其中一个有用的功能是给原始数据类型增加一些方法。 比如，使用 SafeMath 库的时候，我们将使用 using SafeMath for uint256 这样的语法。 SafeMath 库有四个方法 — add， sub， mul， 以及 div。现在我们可以这样来让 uint256 调用这些方法： 12345using SafeMath for uint256;uint256 a &#x3D; 5;uint256 b &#x3D; a.add(3); &#x2F;&#x2F; 5 + 3 &#x3D; 8uint256 c &#x3D; a.mul(2); &#x2F;&#x2F; 5 * 2 &#x3D; 10 Sodility 合约 一份合约就是以太应币应用的基本模块，所有的变量和函数都属于一份合约, 它是所有应用的起点. 版本指令：所有的 Solidity 源码都必须冠以*“version pragma”* — 标明 Solidity 编译器的版本. 以避免将来新的编译器可能破坏你的代码。（etc. pragma solidity ^0.4.19;) 123pragma solidity ^0.4.19;contract ZombieFactory &#123;&#125; OpenZeppelin库的Ownable 合约 1234567891011121314151617181920212223242526272829303132333435&#x2F;** * @title Ownable * @dev The Ownable contract has an owner address, and provides basic authorization control * functions, this simplifies the implementation of &quot;user permissions&quot;. *&#x2F;contract Ownable &#123; address public owner; event OwnershipTransferred(address indexed previousOwner, address indexed newOwner); &#x2F;** * @dev The Ownable constructor sets the original &#96;owner&#96; of the contract to the sender * account. *&#x2F; function Ownable() public &#123; owner &#x3D; msg.sender; &#125; &#x2F;** * @dev Throws if called by any account other than the owner. *&#x2F; modifier onlyOwner() &#123; require(msg.sender &#x3D;&#x3D; owner); _; &#125; &#x2F;** * @dev Allows the current owner to transfer control of the contract to a newOwner. * @param newOwner The address to transfer ownership to. *&#x2F; function transferOwnership(address newOwner) public onlyOwner &#123; require(newOwner !&#x3D; address(0)); OwnershipTransferred(owner, newOwner); owner &#x3D; newOwner; &#125;&#125; 1、构造函数：function Ownable()是一个 _ constructor_ (构造函数)，构造函数不是必须的，它与合约同名，构造函数一生中唯一的一次执行，就是在合约最初被创建的时候。 2、函数修饰符：modifier onlyOwner()。 修饰符跟函数很类似，不过是用来修饰其他已有函数用的， 在其他语句执行前，为它检查下先验条件。 在这个例子中，我们就可以写个修饰符 onlyOwner 检查下调用者，确保只有合约的主人才能运行本函数。 3、indexed 关键字： 数据类型 状态变量 状态变量是被永久地保存在合约中。也就是说它们被写入以太币区块链中. 想象成写入一个数据库。 无符号整数 uint 其值不能是负数，对于有符号的整数存在名为 int 的数据类型。注: Solidity中， uint 实际上是 uint256代名词， 一个256位的无符号整数。你也可以定义位数少的uints — uint8， uint16， uint32 12uint dnaDigits &#x3D; 16;uint dnaModulus &#x3D; 10 ** dnaDigits; 结构体 1234struct Person &#123; uint age; string name;&#125; 数组 Solidity 支持两种数组: 静态 数组和动态 数组 123456&#x2F;&#x2F; 固定长度为2的静态数组:uint[2] fixedArray;&#x2F;&#x2F; 固定长度为5的string类型的静态数组:string[5] stringArray;&#x2F;&#x2F; 动态数组，长度不固定，可以动态添加元素:uint[] dynamicArray; 也可以建立一个结构体类型的数组 12345678Person[] people; &#x2F;&#x2F; 这是动态数组，我们可以不断添加元素&#x2F;&#x2F; 创建一个新的Person:Person satoshi &#x3D; Person(172, &quot;Satoshi&quot;);&#x2F;&#x2F; 将新创建的satoshi添加进people数组:people.push(satoshi);&#x2F;&#x2F;也可以将两步合并people.push(Person(16, &quot;Vitalik&quot;));&#x2F;&#x2F;array.push() 在数组的尾部加入新元素，所以元素在数组中的顺序就是我们添加的顺序 注： 状态变量被永久保存在区块链中。所以在你的合约中创建动态数组来保存成结构的数据是非常有意义的。 公共数组 定义 public 数组, Solidity 会自动创建 getter 方法. 语法如下: 1Zombie[] public zombies; 其它的合约可以从这个数组读取数据（但不能写入数据），所以这在合约中是一个有用的保存公共数据的模式。 映射（Mapping）和地址（Address） Addresses（地址） 以太坊区块链由 _ account _ (账户)组成，一个帐户的余额是 以太 （在以太坊区块链上使用的币种），你可以和其他帐户之间支付和接受以太币，就像银行帐户可以电汇资金到其他银行帐户一样。 在 Solidity 中，有一些全局变量可以被所有函数调用。 其中一个就是 msg.sender，它指的是当前调用者（或智能合约）的 address。 Mapping（映射） 映射本质上是存储和查找数据所用的键-值对。 12mapping (uint &#x3D;&gt; address) public zombieToOwner;mapping (address &#x3D;&gt; uint) ownerZombieCount; Storage与Memory 在 Solidity 中，有两个地方可以存储变量 —— storage 或 memory。 Storage 变量是指永久存储在区块链中的变量。 Memory变量则是临时的，当外部函数对某合约调用完成时，内存型变量即被移除。 你可以把它想象成存储在你电脑的硬盘或是RAM中数据的关系。 默认情况下 Solidity 会自动处理它们。 状态变量（在函数之外声明的变量）默认为“存储”形式，并永久写入区块链；而在函数内部声明的变量是“内存”型的，它们函数调用结束后消失。 然而也有一些情况下，你需要手动声明存储类型，主要用于处理函数内的 _ 结构体 _ 和 _ 数组 _ 时 1234567891011121314151617181920212223Sandwich[] sandwiches;function eatSandwich(uint _index) public &#123; &#x2F;&#x2F; Sandwich mySandwich &#x3D; sandwiches[_index]; &#x2F;&#x2F; ^ 看上去很直接，不过 Solidity 将会给出警告 &#x2F;&#x2F; 告诉你应该明确在这里定义 &#96;storage&#96; 或者 &#96;memory&#96;。 &#x2F;&#x2F; 所以你应该明确定义 &#96;storage&#96;: Sandwich storage mySandwich &#x3D; sandwiches[_index]; &#x2F;&#x2F; ...这样 &#96;mySandwich&#96; 是指向 &#96;sandwiches[_index]&#96;的指针 &#x2F;&#x2F; 在存储里，另外... mySandwich.status &#x3D; &quot;Eaten!&quot;; &#x2F;&#x2F; ...这将永久把 &#96;sandwiches[_index]&#96; 变为区块链上的存储 &#x2F;&#x2F; 如果你只想要一个副本，可以使用&#96;memory&#96;: Sandwich memory anotherSandwich &#x3D; sandwiches[_index + 1]; &#x2F;&#x2F; ...这样 &#96;anotherSandwich&#96; 就仅仅是一个内存里的副本了 &#x2F;&#x2F; 另外 anotherSandwich.status &#x3D; &quot;Eaten!&quot;; &#x2F;&#x2F; ...将仅仅修改临时变量，对 &#96;sandwiches[_index + 1]&#96; 没有任何影响 &#x2F;&#x2F; 不过你可以这样做: sandwiches[_index + 1] &#x3D; anotherSandwich; &#x2F;&#x2F; ...如果你想把副本的改动保存回区块链存储&#125; Keccak256 和 类型转换 Ethereum 内部有一个散列函数keccak256，它用了SHA3版本。一个散列函数基本上就是把一个字符串转换为一个256位的16进制数字。字符串的一个微小变化会引起散列数据极大变化。 1uint rand &#x3D; uint(keccak256(_str)); 时间单位 变量 now 将返回当前的unix时间戳（自1970年1月1日以来经过的秒数）。 Solidity 还包含秒(seconds)，分钟(minutes)，小时(hours)，天(days)，周(weeks) 和 年(years) 等时间单位。它们都会转换成对应的秒数放入 uint 中。 123456789101112uint lastUpdated;&#x2F;&#x2F; 将‘上次更新时间’ 设置为 ‘现在’function updateTimestamp() public &#123; lastUpdated &#x3D; now;&#125;&#x2F;&#x2F; 如果到上次&#96;updateTimestamp&#96; 超过5分钟，返回 &#39;true&#39;&#x2F;&#x2F; 不到5分钟返回 &#39;false&#39;function fiveMinutesHavePassed() public view returns (bool) &#123; return (now &gt;&#x3D; (lastUpdated + 5 minutes));&#125; 函数 在 Solidity 中函数定义的句法如下: 12function createZombie(string _name, uint _dna) &#123; &#125; 注： 习惯上函数里的变量都是以(_)开头 (但不是硬性规定) 以区别全局变量。 require使得函数在执行过程中，当不满足某些条件时抛出错误，并停止执行。在调用一个函数之前，用 require 验证前置条件是非常有必要的。 1require(ownerZombieCount[msg.sender] &#x3D;&#x3D; 0); 公有/私有函数 Solidity 定义的函数的属性默认为公共。 这就意味着任何一方 (或其它合约) 都可以调用你合约里的函数。 将自己的函数定义为私有是一个好的编程习惯，只有当你需要外部世界调用它时才将它设置为公共。 123function _createZombie(string _name, uint _dna) private &#123; zombies.push(Zombie(_name, _dna)); &#125; 这意味着只有我们合约中的其它函数才能够调用这个函数,和函数的参数类似，私有函数的名字用(_)起始。 函数的修饰符 1、可见性修饰符:：决定函数何时和被谁调用：private 意味着它只能被合约内部调用； internal 就像 private 但是也能被继承的合约调用； external 只能从合约外部调用；最后 public 可以在任何地方调用，不管是内部还是外部。 2、状态修饰符：决定函数如何和区块链交互: view 告诉我们运行这个函数不会更改和保存任何数据； pure 告诉我们这个函数不但不会往区块链写数据，它甚至不从区块链读取数据。这两种在被从合约外部调用的时候都不花费任何gas（但是它们在被内部其他函数调用的时候将会耗费gas）。 123456&#x2F;&#x2F;viewfunction sayHello() public view returns (string) &#x2F;&#x2F;purefunction _multiply(uint a, uint b) private pure returns (uint) &#123; return a * b;&#125; 3、自定义的 modifier，我们可以自定义其对函数的约束逻辑。 1234567891011121314&#x2F;&#x2F; 存储用户年龄的映射mapping (uint &#x3D;&gt; uint) public age;&#x2F;&#x2F; 限定用户年龄的修饰符modifier olderThan(uint _age, uint _userId) &#123; require(age[_userId] &gt;&#x3D; _age); _;&#125;&#x2F;&#x2F; 必须年满16周岁才允许开车 (至少在美国是这样的).&#x2F;&#x2F; 我们可以用如下参数调用&#96;olderThan&#96; 修饰符:function driveCar(uint _userId) public olderThan(16, _userId) &#123; &#x2F;&#x2F; 其余的程序逻辑&#125; 4、payable 修饰符 payable方法是让 Solidity 和以太坊变得如此酷的一部分 —— 它们是一种可以接收以太的特殊函数。 12345678contract OnlineStore &#123; function buySomething() external payable &#123; &#x2F;&#x2F; 检查以确定0.001以太发送出去来运行函数: require(msg.value &#x3D;&#x3D; 0.001 ether); &#x2F;&#x2F; 如果为真，一些用来向函数调用者发送数字内容的逻辑 transferThing(msg.sender); &#125;&#125; 在这里，msg.value 是一种可以查看向合约发送了多少以太的方法，另外 ether 是一个內建单元。 这里发生的事是，一些人会从 web3.js 调用这个函数 (从DApp的前端)， 像这样 : 12&#x2F;&#x2F; 假设 &#96;OnlineStore&#96; 在以太坊上指向你的合约:OnlineStore.buySomething().send(from: web3.eth.defaultAccount, value: web3.utils.toWei(0.001)) 提现 12345contract GetPaid is Ownable &#123; function withdraw() external onlyOwner &#123; owner.transfer(this.balance); &#125;&#125; 通过 transfer 函数向一个地址发送以太， 然后 this.balance 将返回当前合约存储了多少以太。 返回值 单个返回值 123function _generateRandomDna(string _str) private returns (uint) &#123; &#x2F;&#x2F; 这里开始 &#125; 多个返回值 123456789101112131415161718function multipleReturns() internal returns(uint a, uint b, uint c) &#123; return (1, 2, 3);&#125;function processMultipleReturns() external &#123; uint a; uint b; uint c; &#x2F;&#x2F; 这样来做批量赋值: (a, b, c) &#x3D; multipleReturns();&#125;&#x2F;&#x2F; 或者如果我们只想返回其中一个变量:function getLastReturnValue() external &#123; uint c; &#x2F;&#x2F; 可以对其他字段留空: (,,c) &#x3D; multipleReturns();&#125; 继承（Inheritance）和引入（Import） 123import &quot;.&#x2F;zombiefactory.sol&quot;;contract ZombieFeeding is ZombieFactory &#123;&#125; 可以实现多继承 12contract ZombieOwnership is ZombieAttack, ERC721 &#123;&#125; For 循环 12345678910111213141516function getEvens() pure external returns(uint[]) &#123; uint[] memory evens &#x3D; new uint[](5); &#x2F;&#x2F; 在新数组中记录序列号 uint counter &#x3D; 0; &#x2F;&#x2F; 在循环从1迭代到10： for (uint i &#x3D; 1; i &lt;&#x3D; 10; i++) &#123; &#x2F;&#x2F; 如果 &#96;i&#96; 是偶数... if (i % 2 &#x3D;&#x3D; 0) &#123; &#x2F;&#x2F; 把它加入偶数数组 evens[counter] &#x3D; i; &#x2F;&#x2F;索引加一， 指向下一个空的‘even’ counter++; &#125; &#125; return evens;&#125; 事件 事件是合约和区块链通讯的一种机制。前端应用“监听”某些事件，并做出反应。 与其他合约的交互 如果我们的合约需要和区块链上的其他的合约会话，则需先定义一个 interface (接口)。 123456789101112131415contract KittyInterface &#123; function getKitty(uint256 _id) external view returns ( bool isGestating, bool isReady, uint256 cooldownIndex, uint256 nextActionAt, uint256 siringWithId, uint256 birthTime, uint256 matronId, uint256 sireId, uint256 generation, uint256 genes );&#125; 这个过程虽然看起来像在定义一个合约，但其实内里不同： 首先，我们只声明了要与之交互的函数 ，在其中我们没有使用到任何其他的函数或状态变量。 其次，我们并没有使用大括号（{ 和 }）定义函数体，我们单单用分号（;）结束了函数声明。这使它看起来像一个合约框架。 编译器就是靠这些特征认出它是一个接口的。 在我们的 app 代码中使用这个接口，合约就知道其他合约的函数是怎样的，应该如何调用，以及可期待什么类型的返回值。 只要将合约的可见性设置为public(公共)或external(外部)，它们就可以与以太坊区块链上的任何其他合约进行交互。 以太坊上的代币 一个 代币 在以太坊基本上就是一个遵循一些共同规则的智能合约——即它实现了所有其他代币合约共享的一组标准函数，例如 transfer(address _to, uint256 _value) 和 balanceOf(address _owner). 在智能合约内部，通常有一个映射， mapping(address =&gt; uint256) balances，用于追踪每个地址还有多少余额。所以基本上一个代币只是一个追踪谁拥有多少该代币的合约，和一些可以让那些用户将他们的代币转移到其他地址的函数。 代币标准：ERC20、ERC721（加密收藏品） ERC271代币是不能互换的，因为每个代币都被认为是唯一且不可分割的。 你只能以整个单位交易它们，并且每个单位都有唯一的 ID。 12345678910contract ERC721 &#123; event Transfer(address indexed _from, address indexed _to, uint256 _tokenId); event Approval(address indexed _owner, address indexed _approved, uint256 _tokenId); function balanceOf(address _owner) public view returns (uint256 _balance); function ownerOf(uint256 _tokenId) public view returns (address _owner); function transfer(address _to, uint256 _tokenId) public; function approve(address _to, uint256 _tokenId) public; function takeOwnership(uint256 _tokenId) public;&#125; ERC721: 转移标准 ERC721 规范有两种不同的方法来转移代币： 1234function transfer(address _to, uint256 _tokenId) public;function approve(address _to, uint256 _tokenId) public;function takeOwnership(uint256 _tokenId) public; 1、第一种方法是代币的拥有者调用transfer 方法，传入他想转移到的 address 和他想转移的代币的 _tokenId。 2、第二种方法是代币拥有者首先调用 approve，然后传入与以上相同的参数。接着，该合约会存储谁被允许提取代币，通常存储到一个 mapping (uint256 =&gt; address) 里。然后，当有人调用 takeOwnership 时，合约会检查 msg.sender 是否得到拥有者的批准来提取代币，如果是，则将代币转移给他。 123456789101112131415161718192021function _transfer(address _from, address _to, uint256 _tokenId) private &#123; ownerZombieCount[_to]++; ownerZombieCount[_from]--; zombieToOwner[_tokenId] &#x3D; _to; Transfer(_from, _to, _tokenId); &#125; function transfer(address _to, uint256 _tokenId) public onlyOwnerOf(_tokenId) &#123; _transfer(msg.sender, _to, _tokenId); &#125; function approve(address _to, uint256 _tokenId) public onlyOwnerOf(_tokenId) &#123; zombieApprovals[_tokenId] &#x3D; _to; Approval(msg.sender, _to, _tokenId); &#125; function takeOwnership(uint256 _tokenId) public &#123; require(zombieApprovals[_tokenId] &#x3D;&#x3D; msg.sender); address owner &#x3D; ownerOf(_tokenId); _transfer(owner, msg.sender, _tokenId); &#125; 应用前端和 Web3.js 什么是 Web3.js? 以太坊网络是由节点组成的，每一个节点都包含了区块链的一份拷贝。当你想要调用一份智能合约的一个方法，你需要从其中一个节点中查找并告诉它:智能合约的地址、你想调用的方法，以及你想传入那个方法的参数。以太坊节点只能识别一种叫做 JSON-RPC 的语言。这种语言直接读起来并不好懂。 幸运的是 Web3.js 把这些令人讨厌的查询语句都隐藏起来了， 所以你只需要与方便易懂的 JavaScript 界面进行交互即可。 12CryptoZombies.methods.createRandomZombie(&quot;Vitalik Nakamoto 🤔&quot;) .send(&#123; from: &quot;0xb60e8dd61c5d32be8058bb8eb970870f07233155&quot;, gas: &quot;3000000&quot; &#125;) 添加Web3.js工具 可以从 github 直接下载压缩后的 .js 文件 然后包含到项目文件中 1&lt;script language&#x3D;&quot;javascript&quot; type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;web3.min.js&quot;&gt;&lt;&#x2F;script&gt; Web3 Provider 以太坊是由共享同一份数据的相同拷贝的 节点 构成的。 在 Web3.js 里设置 Web3 的 Provider（提供者） 告诉我们的代码应该和 哪个节点 交互来处理我们的读写。这就好像在传统的 Web 应用程序中为你的 API 调用设置远程 Web 服务器的网址。 Infura Infura 是一个服务，它维护了很多以太坊节点并提供了一个缓存层来实现高速读取。你可以用他们的 API 来免费访问这个服务。 用 Infura 作为节点提供者，你可以不用自己运营节点就能很可靠地向以太坊发送、接收信息。 你可以通过这样把 Infura 作为你的 Web3 节点提供者： 1var web3 &#x3D; new Web3(new Web3.providers.WebsocketProvider(&quot;wss:&#x2F;&#x2F;mainnet.infura.io&#x2F;ws&quot;)); Metamask Metamask 是 Chrome 和 Firefox 的浏览器扩展， 它能让用户安全地维护他们的以太坊账户和私钥， 并用他们的账户和使用 Web3.js 的网站互动。Metamask 默认使用 Infura 的服务器做为 web3 提供者。 使用 Metamask 的 web3 提供者 123456789101112131415window.addEventListener(&#39;load&#39;, function() &#123; &#x2F;&#x2F; 检查web3是否已经注入到(Mist&#x2F;MetaMask) if (typeof web3 !&#x3D;&#x3D; &#39;undefined&#39;) &#123; &#x2F;&#x2F; 使用 Mist&#x2F;MetaMask 的提供者 web3js &#x3D; new Web3(web3.currentProvider); &#125; else &#123; &#x2F;&#x2F; 处理用户没安装的情况， 比如显示一个消息 &#x2F;&#x2F; 告诉他们要安装 MetaMask 来使用我们的应用 &#125; &#x2F;&#x2F; 现在你可以启动你的应用并自由访问 Web3.js: startApp()&#125;) 与合约对话 Web3.js 需要两个东西来和你的合约对话: 它的地址和它的 ABI。 地址：在你部署智能合约以后，它将获得一个以太坊上的永久地址。 ABI当你编译你的合约向以太坊部署时(我们将在第七课详述)， Solidity 编译器会给你 ABI。将编译了的ABI 并放在名为cryptozombies_abi.js文件中，保存在一个名为 cryptoZombiesABI 的变量中。将cryptozombies_abi.js 包含进我们的项目，我们就能通过那个变量访问 CryptoZombies ABI 。 实例化 Web3.js 12&#x2F;&#x2F; 实例化 myContractvar myContract &#x3D; new web3js.eth.Contract(myABI, myContractAddress); 调用和合约函数 Web3.js 有两个方法来调用我们合约的函数: call and send. Call call 用来调用 view 和 pure 函数。它只运行在本地节点，不会在区块链上创建事务。（view 和 pure 函数是只读的并不会改变区块链的状态。它们也不会消耗任何gas。用户也不会被要求用MetaMask对事务签名。） 123456789function getZombieDetails(id) &#123; return cryptoZombies.methods.zombies(id).call()&#125;&#x2F;&#x2F; 调用函数并做一些其他事情getZombieDetails(15).then(function(result) &#123; console.log(&quot;Zombie 15: &quot; + JSON.stringify(result));&#125;); cryptoZombies.methods.zombies(id).call() 将和 Web3 提供者节点通信，告诉它返回从我们的合约中的 Zombie[] public zombies，id为传入参数的僵尸信息。 注意这是 异步的，就像从外部服务器中调用API。所以 Web3 在这里返回了一个 Promises. (如果你对 JavaScript的 Promises 不了解，最好先去学习一下这方面知识再继续)。 一旦那个 promise 被 resolve, (意味着我们从 Web3 提供者那里获得了响应)，我们的例子代码将执行 then 语句中的代码，在控制台打出 result。 获得 MetaMask中的用户账户 MetaMask 允许用户在扩展中管理多个账户。 我们可以通过这样来获取 web3 变量中激活的当前账户： 1var userAccount &#x3D; web3.eth.accounts[0] 因为用户可以随时在 MetaMask 中切换账户，我们的应用需要监控这个变量，一旦改变就要相应更新界面。例如，若用户的首页展示它们的僵尸大军，当他们在 MetaMask 中切换了账号，我们就需要更新页面来展示新选择的账户的僵尸大军。 我们可以通过 setInterval 方法来做: 123456789var accountInterval &#x3D; setInterval(function() &#123; &#x2F;&#x2F; Check if account has changed if (web3.eth.accounts[0] !&#x3D;&#x3D; userAccount) &#123; userAccount &#x3D; web3.eth.accounts[0]; &#x2F;&#x2F; Call a function to update the UI with the new account getZombiesByOwner(userAccount) .then(displayZombies); &#125; &#125;, 100); 这段代码做的是，每100毫秒检查一次 userAccount 是否还等于 web3.eth.accounts[0] (比如：用户是否还激活了那个账户)。若不等，则将 当前激活用户赋值给 userAccount，然后调用一个函数来更新界面。 Send send 将创建一个事务并改变区块链上的数据。你需要用 send 来调用任何非 view 或者 pure 的函数。（send 一个事务将要求用户支付gas，并会要求弹出对话框请求用户使用 Metamask 对事务签名。在我们使用 Metamask 作为我们的 web3 提供者的时候，所有这一切都会在我们调用 send() 的时候自动发生。而我们自己无需在代码中操心这一切。） 相对 call 函数，send 函数有如下主要区别: 1、send 一个事务需要一个 from 地址来表明谁在调用这个函数（也就是你 Solidity 代码里的 msg.sender )。 我们需要这是我们 DApp 的用户，这样一来 MetaMask 才会弹出提示让他们对事务签名。 2、send 一个事务将花费 gas 3、在用户 send 一个事务到该事务对区块链产生实际影响之间有一个不可忽略的延迟。这是因为我们必须等待事务被包含进一个区块里，以太坊上一个区块的时间平均下来是15秒左右。如果当前在以太坊上有大量挂起事务或者用户发送了过低的 gas 价格，我们的事务可能需要等待数个区块才能被包含进去，往往可能花费数分钟。 所以在我们的代码中我们需要编写逻辑来处理这部分异步特性。 1234567891011121314151617function createRandomZombie(name) &#123; &#x2F;&#x2F; 这将需要一段时间，所以在界面中告诉用户这一点 &#x2F;&#x2F; 事务被发送出去了 $(&quot;#txStatus&quot;).text(&quot;正在区块链上创建僵尸，这将需要一会儿...&quot;); &#x2F;&#x2F; 把事务发送到我们的合约: return cryptoZombies.methods.createRandomZombie(name) .send(&#123; from: userAccount &#125;) .on(&quot;receipt&quot;, function(receipt) &#123; $(&quot;#txStatus&quot;).text(&quot;成功生成了 &quot; + name + &quot;!&quot;); &#x2F;&#x2F; 事务被区块链接受了，重新渲染界面 getZombiesByOwner(userAccount).then(displayZombies); &#125;) .on(&quot;error&quot;, function(error) &#123; &#x2F;&#x2F; 告诉用户合约失败了 $(&quot;#txStatus&quot;).text(error); &#125;);&#125; Web3.js 中需要特殊对待的函数 — payable 函数。 123456789101112function levelUp(zombieId) &#123; $(&quot;#txStatus&quot;).text(&quot;Leveling up your zombie...&quot;); return cryptoZombies.methods.levelUp(zombieId) .send(&#123; from: userAccount, value: web3.utils.toWei(&quot;0.001&quot;, &quot;ether&quot;) &#125;) .on(&quot;receipt&quot;, function(receipt) &#123; $(&quot;#txStatus&quot;).text(&quot;Power overwhelming! Zombie successfully leveled up&quot;); &#125;) .on(&quot;error&quot;, function(error) &#123; $(&quot;#txStatus&quot;).text(error); &#125;); &#125; 订阅合约事件 在 Web3.js里， 你可以订阅 一个事件，这样你的 Web3 提供者可以在每次事件发生后触发你的一些代码逻辑： 12345cryptoZombies.events.NewZombie().on(&quot;data&quot;, function(event) &#123; let zombie &#x3D; event.returnValues; console.log(&quot;一个新僵尸诞生了！&quot;, zombie.zombieId, zombie.name, zombie.dna);&#125;).on(&#39;error&#39;, console.error); 注意这段代码将在 任何 僵尸生成的时候激发一个警告信息——而不仅仅是当前用用户的僵尸。如果我们只想对当前用户发出提醒呢？ 使用indexed 为了筛选仅和当前用户相关的事件，我们的 Solidity 合约将必须使用 indexed 关键字，就像我们在 ERC721 实现中的Transfer 事件中那样： 1event Transfer(address indexed _from, address indexed _to, uint256 _tokenId); 在这种情况下， 因为_from 和 _to 都是 indexed，这就意味着我们可以在前端事件监听中过滤事件 12345cryptoZombies.events.Transfer(&#123; filter: &#123; _to: userAccount &#125; &#125;).on(&quot;data&quot;, function(event) &#123; let data &#x3D; event.returnValues; &#x2F;&#x2F; 当前用户更新了一个僵尸！更新界面来显示&#125;).on(&#39;error&#39;, console.error); 查询过去的事件 我们甚至可以用 getPastEvents 查询过去的事件，并用过滤器 fromBlock 和 toBlock 给 Solidity 一个事件日志的时间范围(“block” 在这里代表以太坊区块编号）： 12345cryptoZombies.getPastEvents(&quot;NewZombie&quot;, &#123; fromBlock: 0, toBlock: &#39;latest&#39; &#125;).then(function(events) &#123; &#x2F;&#x2F; events 是可以用来遍历的 &#96;event&#96; 对象 &#x2F;&#x2F; 这段代码将返回给我们从开始以来创建的僵尸列表&#125;);","categories":[{"name":"区块链","slug":"区块链","permalink":"https://yao-chen-ecnu.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"Solidity","slug":"Solidity","permalink":"https://yao-chen-ecnu.github.io/tags/Solidity/"}]},{"title":"Hello World","slug":"hello-world","date":"2021-03-15T06:57:56.253Z","updated":"2021-03-15T06:57:56.253Z","comments":true,"path":"2021/03/15/hello-world/","link":"","permalink":"https://yao-chen-ecnu.github.io/2021/03/15/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://yao-chen-ecnu.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"区块链+联邦学习论文阅读笔记","slug":"区块链-联邦学习论文阅读笔记","permalink":"https://yao-chen-ecnu.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"name":"区块链","slug":"区块链","permalink":"https://yao-chen-ecnu.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"知识蒸馏","slug":"知识蒸馏","permalink":"https://yao-chen-ecnu.github.io/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"},{"name":"联邦学习","slug":"联邦学习","permalink":"https://yao-chen-ecnu.github.io/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"},{"name":"区块链+联邦学习","slug":"区块链-联邦学习","permalink":"https://yao-chen-ecnu.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"},{"name":"算法","slug":"算法","permalink":"https://yao-chen-ecnu.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"Solidity","slug":"Solidity","permalink":"https://yao-chen-ecnu.github.io/tags/Solidity/"}]}